use serde_json::Value;
use anyhow::{Context, Result};
use tokio::time::{sleep, Instant};
use std::time::Duration;

impl CloudRunBackend {
    async fn get_operation_json(&self, op_name: &str) -> Result<Value> {
        let token = self.get_token().await?;
        let url = format!("https://run.googleapis.com/v2/{}", op_name);

        let res = self
            .client
            .get(&url)
            .bearer_auth(&token)
            .send()
            .await
            .context("błąd GET operation")?;

        if !res.status().is_success() {
            let status = res.status();
            let txt = res.text().await.unwrap_or_default();
            anyhow::bail!("GET operation HTTP error {}: {}", status, txt);
        }

        let v: Value = res.json().await.context("błąd parsowania JSON operation")?;
        Ok(v)
    }

    fn extract_execution_name_from_operation(op: &Value) -> Option<String> {
        // Najczęściej Execution jest w metadata.name albo w response.name (zależnie od LRO)
        // Trzymamy obie ścieżki defensywnie.
        op.get("metadata")
            .and_then(|m| m.get("name"))
            .and_then(|n| n.as_str())
            .map(|s| s.to_string())
            .or_else(|| {
                op.get("response")
                    .and_then(|r| r.get("name"))
                    .and_then(|n| n.as_str())
                    .map(|s| s.to_string())
            })
    }

    async fn wait_for_execution_name(
        &self,
        op_name: &str,
        max_wait: Duration,
        interval: Duration,
    ) -> Result<String> {
        let start = Instant::now();

        loop {
            let op = self.get_operation_json(op_name).await?;

            if let Some(exec_name) = Self::extract_execution_name_from_operation(&op) {
                return Ok(exec_name);
            }

            if start.elapsed() > max_wait {
                anyhow::bail!(
                    "Timeout: nie udało się uzyskać execution name z operation {} w czasie {:?}",
                    op_name,
                    max_wait
                );
            }

            sleep(interval).await;
        }
    }
}

==
impl CloudRunBackend {
    pub async fn start_job(
        &self,
        job_name: &str,
        json_payload: &Value,
        envs: &HashMap<String, String>,
        task_timeout: Duration,
    ) -> Result<String> {
        let token = self.get_token().await?;
        let url = format!("{}/jobs/{}:run", self.jobs_base(), job_name);

        let timeout_secs: i64 = task_timeout.as_secs().try_into().unwrap_or(600);
        let timeout_str = format!("{}s", timeout_secs);

        let env_json: Vec<_> = envs
            .iter()
            .map(|(k, v)| serde_json::json!({ "name": k, "value": v }))
            .collect();

        let body = serde_json::json!({
            "overrides": {
                "containerOverrides": [{
                    "name": "job",
                    "args": [ serde_json::to_string(json_payload)? ],
                    "env": env_json,
                    "clearArgs": false
                }],
                "taskCount": 1,
                "timeout": timeout_str
            }
        });

        info!(%job_name, "Wywołuję Cloud Run Job (:run) ...");

        let res = self
            .client
            .post(&url)
            .bearer_auth(&token)
            .json(&body)
            .send()
            .await
            .context("błąd HTTP POST /jobs/:run")?;

        if !res.status().is_success() {
            let status = res.status();
            let txt = res.text().await.unwrap_or_default();
            anyhow::bail!("Job execution HTTP error {}: {}", status, txt);
        }

        // :run zwraca Operation (LRO), nie execution. :contentReference[oaicite:1]{index=1}
        let v: Value = res.json().await.context("błąd parsowania JSON z :run")?;
        let op_name = v.get("name")
            .and_then(|x| x.as_str())
            .unwrap_or("")
            .to_string();

        if op_name.is_empty() {
            anyhow::bail!(":run nie zwrócił pola 'name' (operation)");
        }

        // Poczekaj aż w operation pojawi się execution name (metadata/response)
        let exec_name = self
            .wait_for_execution_name(&op_name, Duration::from_secs(30), Duration::from_millis(500))
            .await
            .context("nie udało się ustalić execution name dla uruchomienia")?;

        Ok(exec_name)
    }
}

==

impl CloudRunBackend {
    pub async fn poll_until_done(
        &self,
        exec_name: &str,
        max_wait: Duration,
        interval: Duration,
    ) -> Result<String> {
        let start = Instant::now();
        let mut last_state: Option<String> = None;

        loop {
            let status = self.get_execution_status(exec_name).await?;
            let state = status.state.clone().unwrap_or_else(|| "UNKNOWN".to_string());

            if last_state.as_deref() != Some(state.as_str()) {
                debug!(%exec_name, %state, "Execution state changed");
                last_state = Some(state.clone());
            }

            match state.as_str() {
                "SUCCEEDED" => {
                    info!(%exec_name, "Execution SUCCEEDED");
                    return Ok(state);
                }
                "FAILED" | "CANCELLED" => {
                    error!(%exec_name, %state, "Execution FAILED/CANCELLED");
                    anyhow::bail!("Execution {} failed ({})", exec_name, state)
                }
                "RUNNING" | "QUEUED" | "RECONCILING" => {
                    if start.elapsed() > max_wait {
                        error!(%exec_name, "Execution TIMED OUT");
                        anyhow::bail!("Execution {} timed out after {:?}", exec_name, max_wait)
                    }
                    sleep(interval).await;
                }
                _ => {
                    warn!(%exec_name, %state, "Execution w stanie nieznanym — kontynuuję polling");
                    if start.elapsed() > max_wait {
                        anyhow::bail!("Execution {} timed out in UNKNOWN state", exec_name);
                    }
                    sleep(interval).await;
                }
            }
        }
    }
}
==

pub async fn poll_until_done(
    &self,
    exec_name: &str,
    max_wait: Duration,
    interval: Duration,
) -> Result<String> {
    let start = Instant::now();
    let mut last_state: Option<String> = None;

    // tolerancja na UNKNOWN / brak stanu (propagacja, conditions jeszcze nie ma)
    let mut unknown_count: u32 = 0;
    // po ilu "UNKNOWN" z rzędu uznajemy to za błąd (np. 6 * 10s = 60s)
    let max_unknown: u32 = 6;

    loop {
        let status = self.get_execution_status(exec_name).await?;

        // ujednolicenie stanu
        let state = status
            .state
            .clone()
            .unwrap_or_else(|| "UNKNOWN".to_string());

        if last_state.as_deref() != Some(state.as_str()) {
            debug!(%exec_name, %state, "Execution state changed");
            last_state = Some(state.clone());
        }

        match state.as_str() {
            "SUCCEEDED" => {
                info!(%exec_name, "Execution SUCCEEDED");
                return Ok(state);
            }
            "FAILED" | "CANCELLED" => {
                error!(%exec_name, %state, "Execution FAILED/CANCELLED");
                anyhow::bail!("Execution {} failed ({})", exec_name, state);
            }
            "RUNNING" | "QUEUED" | "RECONCILING" => {
                unknown_count = 0;

                if start.elapsed() > max_wait {
                    error!(%exec_name, "Execution TIMED OUT");
                    anyhow::bail!(
                        "Execution {} timed out after {:?}",
                        exec_name,
                        max_wait
                    );
                }
                sleep(interval).await;
            }
            // wszystko inne traktujemy jak "unknown-ish"
            _ => {
                unknown_count += 1;
                warn!(
                    %exec_name,
                    %state,
                    unknown_count,
                    max_unknown,
                    "Execution w stanie nieznanym — kontynuuję polling (tolerancja)"
                );

                if start.elapsed() > max_wait {
                    anyhow::bail!(
                        "Execution {} timed out in state {} after {:?}",
                        exec_name,
                        state,
                        max_wait
                    );
                }

                // jeśli UNKNOWN utrzymuje się zbyt długo -> fail
                if unknown_count >= max_unknown {
                    anyhow::bail!(
                        "Execution {} stays in unknown state '{}' for too long ({} polls)",
                        exec_name,
                        state,
                        unknown_count
                    );
                }

                sleep(interval).await;
            }
        }
    }
}
==
#!/usr/bin/env python3
from __future__ import annotations

import argparse
import hashlib
import json
import re
import shutil
import unicodedata
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from urllib.parse import unquote


PERCENT_ESC_RE = re.compile(r"%[0-9A-Fa-f]{2}")


# ----------------------------
# Simple "tee" logger: stdout + report.txt
# ----------------------------
class Tee:
    def __init__(self, report_path: Path):
        self.report_path = report_path
        self.f = report_path.open("w", encoding="utf-8")

    def close(self):
        try:
            self.f.flush()
        finally:
            self.f.close()

    def println(self, msg: str = ""):
        print(msg)
        self.f.write(msg + "\n")

    def file_action(self, source_name: str, numer_id: str, action: str, details: str = ""):
        # One line per processed file (or skipped/removed)
        # Format: FILE | numer_id=... | action=... | details=...
        line = f"FILE | numer_id={numer_id} | source={source_name} | action={action}"
        if details:
            line += f" | details={details}"
        self.println(line)


@dataclass
class Report:
    # Step 0: dedupe in data/
    data_files_found: int = 0
    data_duplicates_removed: int = 0
    data_duplicates_removed_examples: List[Tuple[str, str]] = None  # (removed, kept)

    # Step 1: copy/rename to processed/
    copied_to_processed: int = 0
    skipped_no_meta: int = 0
    skipped_bad_meta: int = 0
    skipped_extension_mismatch: int = 0
    name_collisions_resolved: int = 0

    # Decoding/validation stats
    url_decoded: int = 0
    decode_failures: int = 0
    leftover_percent: int = 0
    leftover_examples: List[str] = None

    # Optional extra dedupe pass in processed/
    processed_duplicates_removed: int = 0
    processed_duplicates_removed_examples: List[Tuple[str, str]] = None  # (removed, kept)

    def __post_init__(self):
        self.data_duplicates_removed_examples = self.data_duplicates_removed_examples or []
        self.leftover_examples = self.leftover_examples or []
        self.processed_duplicates_removed_examples = self.processed_duplicates_removed_examples or []


def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:
    h = hashlib.sha256()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(chunk_size), b""):
            h.update(chunk)
    return h.hexdigest()


def decode_filename(raw: str, rep: Report, strict: bool) -> Optional[str]:
    raw = raw.strip().replace("\\", "/")
    try:
        decoded = unquote(raw, encoding="utf-8", errors="strict")
        if decoded != raw:
            rep.url_decoded += 1
    except Exception:
        rep.decode_failures += 1
        if strict:
            return None
        decoded = raw

    decoded = Path(decoded).name
    decoded = unicodedata.normalize("NFC", decoded)

    if PERCENT_ESC_RE.search(decoded):
        rep.leftover_percent += 1
        if len(rep.leftover_examples) < 10:
            rep.leftover_examples.append(f"{raw} -> {decoded}")
        if strict:
            return None

    return decoded


def load_meta_filename(meta_path: Path) -> Optional[str]:
    try:
        meta = json.loads(meta_path.read_text(encoding="utf-8"))
    except Exception:
        return None

    full = meta.get("FullFilename")
    if isinstance(full, str) and full.strip():
        return full.strip()

    fn = meta.get("Filename")
    ext = meta.get("Extension")
    if isinstance(fn, str) and fn.strip() and isinstance(ext, str) and ext.strip():
        return f"{fn.strip()}.{ext.strip().lstrip('.')}"
    return None


def safe_target_path(dir_: Path, desired_name: str, overwrite: bool) -> Tuple[Path, bool]:
    """
    If overwrite=True: use desired_name as-is (may overwrite).
    Else if exists: append ' (n)' before extension.
    Returns (path, collision_resolved_flag).
    """
    target = dir_ / desired_name
    if overwrite or not target.exists():
        return target, False

    stem = Path(desired_name).stem
    suffix = Path(desired_name).suffix
    n = 1
    while True:
        candidate = dir_ / f"{stem} ({n}){suffix}"
        if not candidate.exists():
            return candidate, True
        n += 1


# ----------------------------
# Step 0: dedupe SOURCE data/ (before rename/copy)
# Keep deterministically the lexicographically smallest filename in each sha group.
# ----------------------------
def dedupe_data_dir(data_dir: Path, dry_run: bool, debug: bool, rep: Report, log: Tee) -> None:
    files = sorted(p for p in data_dir.iterdir() if p.is_file())
    rep.data_files_found = len(files)

    by_hash: Dict[str, List[Path]] = {}
    for p in files:
        h = sha256_file(p)
        by_hash.setdefault(h, []).append(p)

    for h, group in by_hash.items():
        if len(group) <= 1:
            continue

        # Deterministic keeper: lexicographically smallest filename
        group_sorted = sorted(group, key=lambda p: p.name)
        keep = group_sorted[0]
        dups = group_sorted[1:]

        for dup in dups:
            rep.data_duplicates_removed += 1
            if len(rep.data_duplicates_removed_examples) < 50:
                rep.data_duplicates_removed_examples.append((dup.name, keep.name))

            numer_id = dup.stem
            log.file_action(
                source_name=dup.name,
                numer_id=numer_id,
                action="DEDUP_REMOVED_SOURCE",
                details=f"same_as={keep.name}",
            )
            if debug:
                log.println(f"[DEDUP data/] remove {dup.name} (same as {keep.name})")

            if not dry_run:
                dup.unlink()


# ----------------------------
# Step 1: copy/rename to processed/
# ----------------------------
def process_files(
    data_dir: Path,
    meta_dir: Path,
    processed_dir: Path,
    overwrite: bool,
    strict: bool,
    dry_run: bool,
    debug: bool,
    rep: Report,
    log: Tee,
) -> None:
    processed_dir.mkdir(parents=True, exist_ok=True)

    for src in sorted(p for p in data_dir.iterdir() if p.is_file()):
        numer_id = src.stem
        src_ext = src.suffix.lower().lstrip(".")

        meta_path = meta_dir / f"{numer_id}.json"
        if not meta_path.exists():
            rep.skipped_no_meta += 1
            log.file_action(src.name, numer_id, "SKIP_NO_META", details=f"expected={meta_path.name}")
            if debug:
                log.println(f"[SKIP no-meta] {src.name} -> expected {meta_path.name}")
            continue

        raw_name = load_meta_filename(meta_path)
        if not raw_name:
            rep.skipped_bad_meta += 1
            log.file_action(src.name, numer_id, "SKIP_BAD_META", details="missing FullFilename/Filename+Extension")
            if debug:
                log.println(f"[SKIP bad-meta] {src.name} -> missing FullFilename/Filename+Extension in {meta_path.name}")
            continue

        canonical_name = decode_filename(raw_name, rep, strict)
        if not canonical_name:
            rep.skipped_bad_meta += 1
            log.file_action(src.name, numer_id, "SKIP_DECODE_FAIL", details=f"raw={raw_name}")
            if debug:
                log.println(f"[SKIP decode] {src.name} -> decode/validation failed for: {raw_name}")
            continue

        can_ext = Path(canonical_name).suffix.lower().lstrip(".")
        if can_ext != src_ext:
            rep.skipped_extension_mismatch += 1
            log.file_action(
                src.name,
                numer_id,
                "SKIP_EXT_MISMATCH",
                details=f"src_ext={src_ext}, meta_name={canonical_name}",
            )
            if debug:
                log.println(f"[SKIP ext-mismatch] {src.name} ({src_ext}) vs meta '{canonical_name}' ({can_ext})")
            continue

        target, collided = safe_target_path(processed_dir, canonical_name, overwrite)
        if collided:
            rep.name_collisions_resolved += 1

        action = "WOULD_COPY" if dry_run else ("OVERWRITE" if overwrite and target.exists() else "COPY")
        log.file_action(src.name, numer_id, action, details=f"target={target.name}")

        if debug:
            log.println(f"[{action}] {src.name} -> {target.name}")

        rep.copied_to_processed += 1
        if not dry_run:
            shutil.copy2(src, target)


# ----------------------------
# Optional extra pass: dedupe processed/
# ----------------------------
def dedupe_processed(processed_dir: Path, dry_run: bool, debug: bool, rep: Report, log: Tee) -> None:
    seen: Dict[str, Path] = {}
    for f in sorted(p for p in processed_dir.iterdir() if p.is_file()):
        h = sha256_file(f)
        if h in seen:
            rep.processed_duplicates_removed += 1
            if len(rep.processed_duplicates_removed_examples) < 50:
                rep.processed_duplicates_removed_examples.append((f.name, seen[h].name))

            numer_id = f.stem
            log.file_action(f.name, numer_id, "DEDUP_REMOVED_PROCESSED", details=f"same_as={seen[h].name}")
            if debug:
                log.println(f"[DEDUP processed/] remove {f.name} (same as {seen[h].name})")

            if not dry_run:
                f.unlink()
        else:
            seen[h] = f


def main() -> int:
    ap = argparse.ArgumentParser(
        description=(
            "Step 0: dedupe SOURCE data/ by sha256.\n"
            "Step 1: copy/rename into processed/ using imeta/<id>.json (URL-decode + NFC).\n"
            "Optional: --dedupe-pass does an extra dedupe in processed/.\n"
            "Report is written to report.txt by default."
        )
    )
    ap.add_argument("--data", default="data")
    ap.add_argument("--meta", default="imeta")
    ap.add_argument("--processed", default="processed")
    ap.add_argument("--overwrite", action="store_true")
    ap.add_argument("--strict", action="store_true")
    ap.add_argument("--dry-run", action="store_true")
    ap.add_argument("--debug", action="store_true")
    ap.add_argument("--dedupe-pass", action="store_true")
    args = ap.parse_args()

    data_dir = Path(args.data)
    meta_dir = Path(args.meta)
    processed_dir = Path(args.processed)

    if not data_dir.is_dir():
        raise SystemExit(f"ERROR: data dir not found: {data_dir.resolve()}")
    if not meta_dir.is_dir():
        raise SystemExit(f"ERROR: meta dir not found: {meta_dir.resolve()}")

    report_path = Path("report.txt")
    log = Tee(report_path)

    rep = Report()
    try:
        log.println(f"Data:      {data_dir.resolve()}")
        log.println(f"Meta:      {meta_dir.resolve()}")
        log.println(f"Processed: {processed_dir.resolve()}")
        log.println(
            f"Dry-run:   {args.dry_run}  Strict: {args.strict}  Overwrite: {args.overwrite}  Debug: {args.debug}  Dedupe-pass: {args.dedupe_pass}"
        )
        log.println("")

        # Step 0: dedupe source
        log.println("=== STEP 0: DEDUPE SOURCE data/ (sha256) ===")
        dedupe_data_dir(data_dir, args.dry_run, args.debug, rep, log)
        log.println("")

        # Step 1: rename/copy
        log.println("=== STEP 1: COPY/RENAME into processed/ ===")
        process_files(
            data_dir=data_dir,
            meta_dir=meta_dir,
            processed_dir=processed_dir,
            overwrite=args.overwrite,
            strict=args.strict,
            dry_run=args.dry_run,
            debug=args.debug,
            rep=rep,
            log=log,
        )
        log.println("")

        # Optional dedupe in processed
        if args.dedupe_pass:
            log.println("=== STEP 2: DEDUPE processed/ (sha256) ===")
            processed_dir.mkdir(parents=True, exist_ok=True)
            dedupe_processed(processed_dir, args.dry_run, args.debug, rep, log)
            log.println("")

        # Summary
        log.println("=== REPORT SUMMARY ===")
        log.println(f"Source files found in data/:         {rep.data_files_found}")
        log.println(f"Source duplicates removed:           {rep.data_duplicates_removed}")
        if rep.data_duplicates_removed_examples:
            log.println("Examples (removed -> kept):")
            for r, k in rep.data_duplicates_removed_examples:
                log.println(f"  - {r} -> {k}")
        log.println("")
        log.println(f"Copied to processed/:                {rep.copied_to_processed}")
        log.println(f"Skipped (no meta):                   {rep.skipped_no_meta}")
        log.println(f"Skipped (bad meta/decode):           {rep.skipped_bad_meta}")
        log.println(f"Skipped (extension mismatch):        {rep.skipped_extension_mismatch}")
        log.println(f"Name collisions resolved:            {rep.name_collisions_resolved}")
        log.println("")
        log.println(f"URL decoded names:                   {rep.url_decoded}")
        log.println(f"Decode failures:                     {rep.decode_failures}")
        log.println(f"Leftover %XX after decode:           {rep.leftover_percent}")
        for ex in rep.leftover_examples:
            log.println(f"  - {ex}")
        log.println("")
        if args.dedupe_pass:
            log.println(f"Processed duplicates removed:         {rep.processed_duplicates_removed}")
            if rep.processed_duplicates_removed_examples:
                log.println("Examples (removed -> kept):")
                for r, k in rep.processed_duplicates_removed_examples:
                    log.println(f"  - {r} -> {k}")

    finally:
        log.close()

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
