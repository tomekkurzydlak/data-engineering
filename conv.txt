#!/usr/bin/env python3
from __future__ import annotations

import argparse
import hashlib
import json
import re
import unicodedata
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from urllib.parse import unquote


# Matches any %XX that would indicate still-percent-encoded content
PERCENT_ESC_RE = re.compile(r"%[0-9A-Fa-f]{2}")


@dataclass
class Report:
    # Step 1
    files_found_in_data: int = 0
    renamed: int = 0
    skipped_no_meta: int = 0
    skipped_bad_meta: int = 0
    skipped_missing_fullfilename: int = 0
    skipped_extension_mismatch: int = 0
    name_collisions_resolved: int = 0

    # Decoding/validation stats
    url_decoded: int = 0
    decode_failures: int = 0
    decode_leftover_percent: int = 0
    decode_leftover_percent_examples: List[str] = None

    # Step 2
    duplicates_removed: int = 0
    duplicates: List[Tuple[str, str]] = None  # (removed, kept)

    def __post_init__(self):
        if self.duplicates is None:
            self.duplicates = []
        if self.decode_leftover_percent_examples is None:
            self.decode_leftover_percent_examples = []


def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:
    h = hashlib.sha256()
    with path.open("rb") as f:
        while True:
            chunk = f.read(chunk_size)
            if not chunk:
                break
            h.update(chunk)
    return h.hexdigest()


def safe_target_path(target_dir: Path, desired_name: str) -> Tuple[Path, int]:
    """
    If desired_name already exists, append ' (n)' before extension.
    Returns (final_path, collisions_resolved_flag)
    """
    desired = target_dir / desired_name
    if not desired.exists():
        return desired, 0

    stem = Path(desired_name).stem
    suffix = Path(desired_name).suffix
    n = 1
    while True:
        candidate = target_dir / f"{stem} ({n}){suffix}"
        if not candidate.exists():
            return candidate, 1
        n += 1


def load_meta(meta_path: Path) -> Optional[dict]:
    try:
        return json.loads(meta_path.read_text(encoding="utf-8"))
    except Exception:
        return None


def decode_and_validate_filename(name: str, rep: Report, strict: bool) -> Optional[str]:
    """
    1) percent-decode (URL encoding) using UTF-8
    2) NFC normalize
    3) validate: no leftover %XX sequences
    If strict:
      - on decode error or leftover %XX => return None (skip)
    Else:
      - on decode error => keep original
      - on leftover %XX => keep decoded but report warning
    """
    original = name

    # Normalize any accidental path separators; we only care about basename later anyway.
    name = name.strip().replace("\\", "/")

    decoded = None
    try:
        decoded = unquote(name, encoding="utf-8", errors="strict")
        if decoded != name:
            rep.url_decoded += 1
    except Exception:
        rep.decode_failures += 1
        if strict:
            return None
        decoded = name  # keep as-is

    # take basename only (in case meta contains paths)
    decoded = Path(decoded).name

    # NFC normalize
    decoded = unicodedata.normalize("NFC", decoded)

    # Validation: leftover %XX after decoding
    if PERCENT_ESC_RE.search(decoded):
        rep.decode_leftover_percent += 1
        if len(rep.decode_leftover_percent_examples) < 20:
            rep.decode_leftover_percent_examples.append(f"{original} -> {decoded}")
        if strict:
            return None

    return decoded


def pick_meta_filename(meta: dict) -> Optional[str]:
    # You said Filename and FullFilename are effectively the same source.
    # We'll use FullFilename if present; else try Filename + Extension.
    full = meta.get("FullFilename")
    if isinstance(full, str) and full.strip():
        return full.strip()

    fn = meta.get("Filename")
    ext = meta.get("Extension")
    if isinstance(fn, str) and fn.strip() and isinstance(ext, str) and ext.strip():
        ext = ext.strip().lstrip(".")
        return f"{fn.strip()}.{ext}"

    return None


def rename_files(data_dir: Path, meta_dir: Path, dry_run: bool, strict: bool) -> Report:
    rep = Report()
    files = sorted([p for p in data_dir.iterdir() if p.is_file()])

    for src_path in files:
        rep.files_found_in_data += 1

        numer_id = src_path.stem  # <numer_id> from <numer_id>.<ext>
        src_ext = src_path.suffix.lower().lstrip(".")  # ext without dot

        meta_path = meta_dir / f"{numer_id}.json"
        if not meta_path.exists():
            rep.skipped_no_meta += 1
            continue

        meta = load_meta(meta_path)
        if meta is None:
            rep.skipped_bad_meta += 1
            continue

        meta_full = pick_meta_filename(meta)
        if not meta_full:
            rep.skipped_missing_fullfilename += 1
            continue

        decoded_full = decode_and_validate_filename(meta_full, rep, strict=strict)
        if not decoded_full:
            # strict mode: reject broken decoding/validation
            rep.skipped_bad_meta += 1
            continue

        # If meta says extension explicitly, ensure it matches the file in data/ (safety).
        meta_ext = meta.get("Extension")
        if isinstance(meta_ext, str) and meta_ext.strip():
            meta_ext_norm = meta_ext.strip().lower().lstrip(".")
            if meta_ext_norm != src_ext:
                rep.skipped_extension_mismatch += 1
                continue

        # Also ensure decoded_full extension matches src file extension; if not, skip.
        decoded_ext = Path(decoded_full).suffix.lower().lstrip(".")
        if decoded_ext and decoded_ext != src_ext:
            rep.skipped_extension_mismatch += 1
            continue

        target_path, collided = safe_target_path(data_dir, decoded_full)
        if collided:
            rep.name_collisions_resolved += 1

        # If already correct name (same path), do nothing
        if target_path.resolve() == src_path.resolve():
            continue

        rep.renamed += 1
        if not dry_run:
            src_path.rename(target_path)

    return rep


def dedupe_files_in_data_dir(data_dir: Path, dry_run: bool, rep: Report) -> None:
    files = sorted([p for p in data_dir.iterdir() if p.is_file()])
    seen: Dict[str, Path] = {}

    for p in files:
        h = sha256_file(p)
        if h in seen:
            kept = seen[h]
            rep.duplicates_removed += 1
            rep.duplicates.append((p.name, kept.name))
            if not dry_run:
                p.unlink()
        else:
            seen[h] = p


def main() -> int:
    ap = argparse.ArgumentParser(
        description=(
            "Step 1: rename data/<numer_id>.<ext> using imeta/<numer_id>.json FullFilename (URL-decode + NFC).\n"
            "Step 2: remove binary duplicates in data/ (sha256).\n"
            "Step 3: print report."
        )
    )
    ap.add_argument("--data", default="data", help="Directory with binary files (default: data)")
    ap.add_argument("--meta", default="imeta", help="Directory with meta JSON files (default: imeta)")
    ap.add_argument("--dry-run", action="store_true", help="Do not modify files, only print what would happen")
    ap.add_argument(
        "--strict",
        action="store_true",
        help="Strict decoding/validation: skip files if URL-decoding fails or leftover %%XX remains after decode",
    )
    args = ap.parse_args()

    data_dir = Path(args.data)
    meta_dir = Path(args.meta)

    if not data_dir.is_dir():
        raise SystemExit(f"ERROR: data dir not found: {data_dir}")
    if not meta_dir.is_dir():
        raise SystemExit(f"ERROR: meta dir not found: {meta_dir}")

    rep = rename_files(data_dir, meta_dir, dry_run=args.dry_run, strict=args.strict)
    dedupe_files_in_data_dir(data_dir, dry_run=args.dry_run, rep=rep)

    print("\n=== REPORT ===")
    print(f"Data dir:  {data_dir.resolve()}")
    print(f"Meta dir:  {meta_dir.resolve()}")
    print(f"Dry-run:   {args.dry_run}")
    print(f"Strict:    {args.strict}")
    print("")
    print(f"Files found in data/:            {rep.files_found_in_data}")
    print(f"Renamed:                        {rep.renamed}")
    print(f"Skipped (no meta):              {rep.skipped_no_meta}")
    print(f"Skipped (bad meta / decode):    {rep.skipped_bad_meta}")
    print(f"Skipped (missing FullFilename): {rep.skipped_missing_fullfilename}")
    print(f"Skipped (extension mismatch):   {rep.skipped_extension_mismatch}")
    print(f"Name collisions resolved:       {rep.name_collisions_resolved}")
    print("")
    print(f"URL-decoded filenames:          {rep.url_decoded}")
    print(f"Decode failures:                {rep.decode_failures}")
    print(f"Leftover %XX after decode:      {rep.decode_leftover_percent}")
    if rep.decode_leftover_percent_examples:
        print("Examples (original -> decoded):")
        for ex in rep.decode_leftover_percent_examples:
            print(f"- {ex}")
    print("")
    print(f"Duplicates removed (sha256):    {rep.duplicates_removed}")
    if rep.duplicates_removed:
        print("\nRemoved duplicates (removed -> kept):")
        for removed, kept in rep.duplicates:
            print(f"- {removed} -> {kept}")

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
