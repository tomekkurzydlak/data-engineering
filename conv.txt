#!/usr/bin/env python3
from __future__ import annotations

import argparse
import hashlib
import json
import re
import unicodedata
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from urllib.parse import unquote


PERCENT_ESC_RE = re.compile(r"%[0-9A-Fa-f]{2}")


@dataclass
class Report:
    files_found_in_data: int = 0
    copied_to_processed: int = 0
    skipped_no_meta: int = 0
    skipped_bad_meta: int = 0
    skipped_extension_mismatch: int = 0
    name_collisions_resolved: int = 0

    url_decoded: int = 0
    decode_failures: int = 0
    leftover_percent: int = 0
    leftover_examples: List[str] = None

    duplicates_removed: int = 0
    duplicates: List[Tuple[str, str]] = None

    def __post_init__(self):
        self.leftover_examples = self.leftover_examples or []
        self.duplicates = self.duplicates or []


def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:
    h = hashlib.sha256()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(chunk_size), b""):
            h.update(chunk)
    return h.hexdigest()


def safe_target_path(dir_: Path, name: str, overwrite: bool) -> Tuple[Path, bool]:
    target = dir_ / name
    if overwrite or not target.exists():
        return target, False

    stem = Path(name).stem
    suffix = Path(name).suffix
    n = 1
    while True:
        candidate = dir_ / f"{stem} ({n}){suffix}"
        if not candidate.exists():
            return candidate, True
        n += 1


def decode_filename(raw: str, rep: Report, strict: bool) -> Optional[str]:
    raw = raw.strip().replace("\\", "/")

    try:
        decoded = unquote(raw, encoding="utf-8", errors="strict")
        if decoded != raw:
            rep.url_decoded += 1
    except Exception:
        rep.decode_failures += 1
        if strict:
            return None
        decoded = raw

    decoded = Path(decoded).name
    decoded = unicodedata.normalize("NFC", decoded)

    if PERCENT_ESC_RE.search(decoded):
        rep.leftover_percent += 1
        if len(rep.leftover_examples) < 10:
            rep.leftover_examples.append(decoded)
        if strict:
            return None

    return decoded


def load_meta_filename(meta_path: Path) -> Optional[str]:
    try:
        meta = json.loads(meta_path.read_text(encoding="utf-8"))
    except Exception:
        return None

    full = meta.get("FullFilename")
    if isinstance(full, str) and full.strip():
        return full.strip()

    fn = meta.get("Filename")
    ext = meta.get("Extension")
    if isinstance(fn, str) and isinstance(ext, str):
        return f"{fn.strip()}.{ext.strip().lstrip('.')}"

    return None


def process_files(
    data_dir: Path,
    meta_dir: Path,
    processed_dir: Path,
    overwrite: bool,
    strict: bool,
    dry_run: bool,
) -> Report:
    rep = Report()
    processed_dir.mkdir(parents=True, exist_ok=True)

    for src in sorted(p for p in data_dir.iterdir() if p.is_file()):
        rep.files_found_in_data += 1
        numer_id = src.stem
        src_ext = src.suffix.lower().lstrip(".")

        meta_path = meta_dir / f"{numer_id}.json"
        if not meta_path.exists():
            rep.skipped_no_meta += 1
            continue

        meta_name_raw = load_meta_filename(meta_path)
        if not meta_name_raw:
            rep.skipped_bad_meta += 1
            continue

        decoded_name = decode_filename(meta_name_raw, rep, strict)
        if not decoded_name:
            rep.skipped_bad_meta += 1
            continue

        if Path(decoded_name).suffix.lower().lstrip(".") != src_ext:
            rep.skipped_extension_mismatch += 1
            continue

        target, collided = safe_target_path(processed_dir, decoded_name, overwrite)
        if collided:
            rep.name_collisions_resolved += 1

        rep.copied_to_processed += 1
        if not dry_run:
            target.write_bytes(src.read_bytes())

    return rep


def dedupe_processed(processed_dir: Path, dry_run: bool, rep: Report):
    seen: Dict[str, Path] = {}
    for f in sorted(p for p in processed_dir.iterdir() if p.is_file()):
        h = sha256_file(f)
        if h in seen:
            rep.duplicates_removed += 1
            rep.duplicates.append((f.name, seen[h].name))
            if not dry_run:
                f.unlink()
        else:
            seen[h] = f


def main() -> int:
    ap = argparse.ArgumentParser("Rename + normalize + dedupe into processed/")
    ap.add_argument("--data", default="data")
    ap.add_argument("--meta", default="imeta")
    ap.add_argument("--processed", default="processed")
    ap.add_argument("--overwrite", action="store_true")
    ap.add_argument("--strict", action="store_true")
    ap.add_argument("--dry-run", action="store_true")
    args = ap.parse_args()

    rep = process_files(
        Path(args.data),
        Path(args.meta),
        Path(args.processed),
        overwrite=args.overwrite,
        strict=args.strict,
        dry_run=args.dry_run,
    )

    dedupe_processed(Path(args.processed), args.dry_run, rep)

    print("\n=== REPORT ===")
    print(f"Data files found:        {rep.files_found_in_data}")
    print(f"Copied to processed/:   {rep.copied_to_processed}")
    print(f"Skipped (no meta):      {rep.skipped_no_meta}")
    print(f"Skipped (bad meta):     {rep.skipped_bad_meta}")
    print(f"Skipped (ext mismatch): {rep.skipped_extension_mismatch}")
    print(f"Collisions resolved:    {rep.name_collisions_resolved}")
    print()
    print(f"URL decoded names:      {rep.url_decoded}")
    print(f"Decode failures:       {rep.decode_failures}")
    print(f"Leftover %XX:          {rep.leftover_percent}")
    for ex in rep.leftover_examples:
        print(f"  - {ex}")
    print()
    print(f"Duplicates removed:     {rep.duplicates_removed}")
    for r, k in rep.duplicates:
        print(f"  - {r} -> {k}")

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
