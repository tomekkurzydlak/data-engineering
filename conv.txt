#!/usr/bin/env python3
from __future__ import annotations

import argparse
import hashlib
import json
import re
import unicodedata
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from urllib.parse import unquote


PERCENT_ESC_RE = re.compile(r"%[0-9A-Fa-f]{2}")


@dataclass
class Report:
    files_found_in_data: int = 0
    copied_to_processed: int = 0
    skipped_no_meta: int = 0
    skipped_bad_meta: int = 0
    skipped_extension_mismatch: int = 0
    name_collisions_resolved: int = 0

    url_decoded: int = 0
    decode_failures: int = 0
    leftover_percent: int = 0
    leftover_examples: List[str] = None

    duplicates_removed: int = 0
    duplicates: List[Tuple[str, str]] = None

    def __post_init__(self):
        self.leftover_examples = self.leftover_examples or []
        self.duplicates = self.duplicates or []


def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:
    h = hashlib.sha256()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(chunk_size), b""):
            h.update(chunk)
    return h.hexdigest()


def safe_target_path(dir_: Path, name: str, overwrite: bool) -> Tuple[Path, bool]:
    target = dir_ / name
    if overwrite or not target.exists():
        return target, False

    stem = Path(name).stem
    suffix = Path(name).suffix
    n = 1
    while True:
        candidate = dir_ / f"{stem} ({n}){suffix}"
        if not candidate.exists():
            return candidate, True
        n += 1


def decode_filename(raw: str, rep: Report, strict: bool) -> Optional[str]:
    raw = raw.strip().replace("\\", "/")

    try:
        decoded = unquote(raw, encoding="utf-8", errors="strict")
        if decoded != raw:
            rep.url_decoded += 1
    except Exception:
        rep.decode_failures += 1
        if strict:
            return None
        decoded = raw

    decoded = Path(decoded).name
    decoded = unicodedata.normalize("NFC", decoded)

    if PERCENT_ESC_RE.search(decoded):
        rep.leftover_percent += 1
        if len(rep.leftover_examples) < 10:
            rep.leftover_examples.append(decoded)
        if strict:
            return None

    return decoded


def load_meta_filename(meta_path: Path) -> Optional[str]:
    try:
        meta = json.loads(meta_path.read_text(encoding="utf-8"))
    except Exception:
        return None

    full = meta.get("FullFilename")
    if isinstance(full, str) and full.strip():
        return full.strip()

    fn = meta.get("Filename")
    ext = meta.get("Extension")
    if isinstance(fn, str) and isinstance(ext, str):
        return f"{fn.strip()}.{ext.strip().lstrip('.')}"

    return None


def process_files(
    data_dir: Path,
    meta_dir: Path,
    processed_dir: Path,
    overwrite: bool,
    strict: bool,
    dry_run: bool,
) -> Report:
    rep = Report()
    processed_dir.mkdir(parents=True, exist_ok=True)

    for src in sorted(p for p in data_dir.iterdir() if p.is_file()):
        rep.files_found_in_data += 1
        numer_id = src.stem
        src_ext = src.suffix.lower().lstrip(".")

        meta_path = meta_dir / f"{numer_id}.json"
        if not meta_path.exists():
            rep.skipped_no_meta += 1
            continue

        meta_name_raw = load_meta_filename(meta_path)
        if not meta_name_raw:
            rep.skipped_bad_meta += 1
            continue

        decoded_name = decode_filename(meta_name_raw, rep, strict)
        if not decoded_name:
            rep.skipped_bad_meta += 1
            continue

        if Path(decoded_name).suffix.lower().lstrip(".") != src_ext:
            rep.skipped_extension_mismatch += 1
            continue

        target, collided = safe_target_path(processed_dir, decoded_name, overwrite)
        if collided:
            rep.name_collisions_resolved += 1

        rep.copied_to_processed += 1
        if not dry_run:
            target.write_bytes(src.read_bytes())

    return rep


def dedupe_processed(processed_dir: Path, dry_run: bool, rep: Report):
    seen: Dict[str, Path] = {}
    for f in sorted(p for p in processed_dir.iterdir() if p.is_file()):
        h = sha256_file(f)
        if h in seen:
            rep.duplicates_removed += 1
            rep.duplicates.append((f.name, seen[h].name))
            if not dry_run:
                f.unlink()
        else:
            seen[h] = f


def main() -> int:
    ap = argparse.ArgumentParser("Rename + normalize + dedupe into processed/")
    ap.add_argument("--data", default="data")
    ap.add_argument("--meta", default="imeta")
    ap.add_argument("--processed", default="processed")
    ap.add_argument("--overwrite", action="store_true")
    ap.add_argument("--strict", action="store_true")
    ap.add_argument("--dry-run", action="store_true")
    args = ap.parse_args()

    rep = process_files(
        Path(args.data),
        Path(args.meta),
        Path(args.processed),
        overwrite=args.overwrite,
        strict=args.strict,
        dry_run=args.dry_run,
    )

    dedupe_processed(Path(args.processed), args.dry_run, rep)

    print("\n=== REPORT ===")
    print(f"Data files found:        {rep.files_found_in_data}")
    print(f"Copied to processed/:   {rep.copied_to_processed}")
    print(f"Skipped (no meta):      {rep.skipped_no_meta}")
    print(f"Skipped (bad meta):     {rep.skipped_bad_meta}")
    print(f"Skipped (ext mismatch): {rep.skipped_extension_mismatch}")
    print(f"Collisions resolved:    {rep.name_collisions_resolved}")
    print()
    print(f"URL decoded names:      {rep.url_decoded}")
    print(f"Decode failures:       {rep.decode_failures}")
    print(f"Leftover %XX:          {rep.leftover_percent}")
    for ex in rep.leftover_examples:
        print(f"  - {ex}")
    print()
    print(f"Duplicates removed:     {rep.duplicates_removed}")
    for r, k in rep.duplicates:
        print(f"  - {r} -> {k}")

    return 0


if __name__ == "__main__":
    raise SystemExit(main())

===

#!/usr/bin/env python3
from __future__ import annotations

import argparse
import hashlib
import json
import re
import shutil
import unicodedata
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from urllib.parse import unquote

PERCENT_ESC_RE = re.compile(r"%[0-9A-Fa-f]{2}")


# ----------------------------
# Tee logger: console + report.txt
# ----------------------------
class Tee:
    def __init__(self, report_path: Path):
        self.f = report_path.open("w", encoding="utf-8")

    def close(self):
        try:
            self.f.flush()
        finally:
            self.f.close()

    def println(self, msg: str = ""):
        print(msg)
        self.f.write(msg + "\n")

    def file_action(self, filename: str, numer_id: str, action: str, details: str = ""):
        line = f"FILE | numer_id={numer_id} | file={filename} | action={action}"
        if details:
            line += f" | details={details}"
        self.println(line)


@dataclass
class Report:
    # Step 0: copy raw
    data_files_found: int = 0
    copied_raw_to_processed: int = 0

    # Optional: dedupe in source (only with --overwrite)
    data_duplicates_removed: int = 0
    data_duplicates_removed_examples: List[Tuple[str, str]] = None

    # Step 1: dedupe in processed (raw names)
    processed_duplicates_removed: int = 0
    processed_duplicates_removed_examples: List[Tuple[str, str]] = None

    # Step 2: rename in processed
    renamed_in_processed: int = 0
    skipped_no_meta: int = 0
    skipped_bad_meta: int = 0
    skipped_extension_mismatch: int = 0
    rename_collisions_resolved: int = 0

    # decoding/validation stats
    url_decoded: int = 0
    decode_failures: int = 0
    leftover_percent: int = 0
    leftover_examples: List[str] = None

    def __post_init__(self):
        self.data_duplicates_removed_examples = self.data_duplicates_removed_examples or []
        self.processed_duplicates_removed_examples = self.processed_duplicates_removed_examples or []
        self.leftover_examples = self.leftover_examples or []


def sha256_file(path: Path, chunk_size: int = 1024 * 1024) -> str:
    h = hashlib.sha256()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(chunk_size), b""):
            h.update(chunk)
    return h.hexdigest()


def load_meta_filename(meta_path: Path) -> Optional[str]:
    try:
        meta = json.loads(meta_path.read_text(encoding="utf-8"))
    except Exception:
        return None

    full = meta.get("FullFilename")
    if isinstance(full, str) and full.strip():
        return full.strip()

    fn = meta.get("Filename")
    ext = meta.get("Extension")
    if isinstance(fn, str) and fn.strip() and isinstance(ext, str) and ext.strip():
        return f"{fn.strip()}.{ext.strip().lstrip('.')}"
    return None


def decode_filename(raw: str, rep: Report, strict: bool) -> Optional[str]:
    raw = raw.strip().replace("\\", "/")
    try:
        decoded = unquote(raw, encoding="utf-8", errors="strict")
        if decoded != raw:
            rep.url_decoded += 1
    except Exception:
        rep.decode_failures += 1
        if strict:
            return None
        decoded = raw

    decoded = Path(decoded).name
    decoded = unicodedata.normalize("NFC", decoded)

    if PERCENT_ESC_RE.search(decoded):
        rep.leftover_percent += 1
        if len(rep.leftover_examples) < 10:
            rep.leftover_examples.append(f"{raw} -> {decoded}")
        if strict:
            return None

    return decoded


def safe_target_path(dir_: Path, desired_name: str) -> Tuple[Path, bool]:
    """
    If desired_name already exists, append ' (n)' before extension.
    IMPORTANT: desired_name is always preferred (canonical).
    Returns (path, collision_resolved_flag)
    """
    target = dir_ / desired_name
    if not target.exists():
        return target, False

    stem = Path(desired_name).stem
    suffix = Path(desired_name).suffix
    n = 1
    while True:
        candidate = dir_ / f"{stem} ({n}){suffix}"
        if not candidate.exists():
            return candidate, True
        n += 1


# ----------------------------
# Step 0: copy raw files as-is (numer_id.*) to processed/
# In safe mode (no overwrite): do NOT overwrite existing files in processed.
# In overwrite mode: overwrite processed copy too.
# ----------------------------
def copy_raw_to_processed(
    data_dir: Path, processed_dir: Path, overwrite: bool, dry_run: bool, debug: bool, log: Tee, rep: Report
) -> None:
    processed_dir.mkdir(parents=True, exist_ok=True)
    files = sorted(p for p in data_dir.iterdir() if p.is_file())
    rep.data_files_found = len(files)

    for src in files:
        numer_id = src.stem
        dst = processed_dir / src.name

        if dst.exists() and not overwrite:
            # keep existing (safety), no action
            log.file_action(src.name, numer_id, "SKIP_COPY_EXISTS", details=f"processed_has={dst.name}")
            if debug:
                log.println(f"[SKIP copy] {src.name} -> {dst.name} exists (no --overwrite)")
            continue

        log.file_action(src.name, numer_id, "COPY_RAW" if not dry_run else "WOULD_COPY_RAW", details=f"to={dst.name}")
        if debug:
            log.println(f"[COPY raw] {src.name} -> {dst.name}")

        rep.copied_raw_to_processed += 1
        if not dry_run:
            shutil.copy2(src, dst)


# ----------------------------
# Dedupe helper: remove binary duplicates in a directory
# Keep deterministically the lexicographically smallest filename as canonical survivor.
# This aligns with your "keep the original, not (1)" rule.
# ----------------------------
def dedupe_dir(
    dir_: Path,
    dry_run: bool,
    debug: bool,
    log: Tee,
    rep_count_attr: str,
    rep_examples_attr: str,
    action_prefix: str,
) -> None:
    files = sorted(p for p in dir_.iterdir() if p.is_file())

    by_hash: Dict[str, List[Path]] = {}
    for p in files:
        h = sha256_file(p)
        by_hash.setdefault(h, []).append(p)

    removed_count = 0
    removed_examples: List[Tuple[str, str]] = []

    for h, group in by_hash.items():
        if len(group) <= 1:
            continue

        group_sorted = sorted(group, key=lambda p: p.name)
        keep = group_sorted[0]
        for dup in group_sorted[1:]:
            removed_count += 1
            if len(removed_examples) < 50:
                removed_examples.append((dup.name, keep.name))

            numer_id = dup.stem
            log.file_action(dup.name, numer_id, f"{action_prefix}_REMOVED", details=f"same_as={keep.name}")
            if debug:
                log.println(f"[DEDUP {dir_.name}] remove {dup.name} (same as {keep.name})")

            if not dry_run:
                dup.unlink()

    setattr(rep, rep_count_attr, removed_count)
    setattr(rep, rep_examples_attr, removed_examples)


# ----------------------------
# Step 2: rename in processed based on meta
# Prefer canonical name (no suffix). If collision -> other file gets (1).
# ----------------------------
def rename_in_processed(
    processed_dir: Path,
    meta_dir: Path,
    overwrite: bool,
    strict: bool,
    dry_run: bool,
    debug: bool,
    log: Tee,
    rep: Report,
) -> None:
    # Work on a snapshot list to avoid issues while renaming
    files = sorted(p for p in processed_dir.iterdir() if p.is_file())

    for src in files:
        numer_id = src.stem
        src_ext = src.suffix.lower().lstrip(".")

        meta_path = meta_dir / f"{numer_id}.json"
        if not meta_path.exists():
            rep.skipped_no_meta += 1
            log.file_action(src.name, numer_id, "SKIP_NO_META", details=f"expected={meta_path.name}")
            if debug:
                log.println(f"[SKIP no-meta] {src.name} -> expected {meta_path.name}")
            continue

        raw_name = load_meta_filename(meta_path)
        if not raw_name:
            rep.skipped_bad_meta += 1
            log.file_action(src.name, numer_id, "SKIP_BAD_META", details="missing FullFilename/Filename+Extension")
            if debug:
                log.println(f"[SKIP bad-meta] {src.name} -> missing FullFilename/Filename+Extension in {meta_path.name}")
            continue

        canonical_name = decode_filename(raw_name, rep, strict)
        if not canonical_name:
            rep.skipped_bad_meta += 1
            log.file_action(src.name, numer_id, "SKIP_DECODE_FAIL", details=f"raw={raw_name}")
            if debug:
                log.println(f"[SKIP decode] {src.name} -> decode/validation failed for: {raw_name}")
            continue

        can_ext = Path(canonical_name).suffix.lower().lstrip(".")
        if can_ext != src_ext:
            rep.skipped_extension_mismatch += 1
            log.file_action(src.name, numer_id, "SKIP_EXT_MISMATCH", details=f"src_ext={src_ext}, meta={canonical_name}")
            if debug:
                log.println(f"[SKIP ext-mismatch] {src.name} ({src_ext}) vs meta '{canonical_name}' ({can_ext})")
            continue

        desired = processed_dir / canonical_name

        # If desired exists:
        # - if it's literally the same file path -> nothing
        # - else: we always keep desired as canonical, and move the current file to a safe variant
        if desired.resolve() == src.resolve():
            log.file_action(src.name, numer_id, "ALREADY_CANONICAL", details=f"name={canonical_name}")
            continue

        if desired.exists():
            # Collision: canonical must survive. Move current to (1).
            target, collided = safe_target_path(processed_dir, canonical_name)
            # safe_target_path returns canonical if free; but canonical exists, so it will return (1)
            rep.rename_collisions_resolved += 1
            if debug:
                log.println(f"[RENAME collision] {src.name} -> {target.name} (canonical {canonical_name} exists)")

            log.file_action(src.name, numer_id, "RENAME_COLLISION", details=f"to={target.name}, canonical={canonical_name}")
            if not dry_run:
                src.rename(target)
            rep.renamed_in_processed += 1
        else:
            if debug:
                log.println(f"[RENAME] {src.name} -> {canonical_name}")
            log.file_action(src.name, numer_id, "RENAME" if not dry_run else "WOULD_RENAME", details=f"to={canonical_name}")
            if not dry_run:
                src.rename(desired)
            rep.renamed_in_processed += 1


def main() -> int:
    ap = argparse.ArgumentParser(
        description=(
            "Safe mode (default): copy raw data/* to processed/*, dedupe + rename ONLY in processed.\n"
            "Overwrite mode (--overwrite): also allows dedupe in data/ (source), and overwriting processed copies.\n"
            "Always writes report to report.txt."
        )
    )
    ap.add_argument("--data", default="data")
    ap.add_argument("--meta", default="imeta")
    ap.add_argument("--processed", default="processed")
    ap.add_argument("--overwrite", action="store_true")
    ap.add_argument("--strict", action="store_true")
    ap.add_argument("--dry-run", action="store_true")
    ap.add_argument("--debug", action="store_true")
    args = ap.parse_args()

    data_dir = Path(args.data)
    meta_dir = Path(args.meta)
    processed_dir = Path(args.processed)

    if not data_dir.is_dir():
        raise SystemExit(f"ERROR: data dir not found: {data_dir.resolve()}")
    if not meta_dir.is_dir():
        raise SystemExit(f"ERROR: meta dir not found: {meta_dir.resolve()}")

    log = Tee(Path("report.txt"))
    rep = Report()
    try:
        log.println(f"Data:      {data_dir.resolve()}")
        log.println(f"Meta:      {meta_dir.resolve()}")
        log.println(f"Processed: {processed_dir.resolve()}")
        log.println(f"Dry-run:   {args.dry_run}  Strict: {args.strict}  Overwrite: {args.overwrite}  Debug: {args.debug}")
        log.println("")

        # 0) Copy raw snapshot to processed
        log.println("=== STEP 0: COPY RAW data/ -> processed/ (no renames) ===")
        copy_raw_to_processed(data_dir, processed_dir, args.overwrite, args.dry_run, args.debug, log, rep)
        log.println("")

        # 1) If overwrite: allow dedupe in source too (aggressive mode)
        if args.overwrite:
            log.println("=== STEP 1A: DEDUPE SOURCE data/ (sha256) [only with --overwrite] ===")
            dedupe_dir(
                data_dir,
                args.dry_run,
                args.debug,
                log,
                rep_count_attr="data_duplicates_removed",
                rep_examples_attr="data_duplicates_removed_examples",
                action_prefix="DEDUP_SOURCE",
            )
            log.println("")

        # 2) Dedupe processed (always)
        log.println("=== STEP 1B: DEDUPE processed/ (sha256) ===")
        dedupe_dir(
            processed_dir,
            args.dry_run,
            args.debug,
            log,
            rep_count_attr="processed_duplicates_removed",
            rep_examples_attr="processed_duplicates_removed_examples",
            action_prefix="DEDUP_PROCESSED",
        )
        log.println("")

        # 3) Rename in processed (always)
        log.println("=== STEP 2: RENAME in processed/ from meta ===")
        rename_in_processed(
            processed_dir,
            meta_dir,
            overwrite=args.overwrite,
            strict=args.strict,
            dry_run=args.dry_run,
            debug=args.debug,
            log=log,
            rep=rep,
        )
        log.println("")

        # Summary
        log.println("=== REPORT SUMMARY ===")
        log.println(f"Source files found in data/:               {rep.data_files_found}")
        log.println(f"Copied raw to processed/:                  {rep.copied_raw_to_processed}")
        log.println("")
        log.println(f"Processed duplicates removed:              {rep.processed_duplicates_removed}")
        if rep.processed_duplicates_removed_examples:
            log.println("Examples (removed -> kept):")
            for r, k in rep.processed_duplicates_removed_examples:
                log.println(f"  - {r} -> {k}")
        log.println("")
        log.println(f"Renamed in processed/:                     {rep.renamed_in_processed}")
        log.println(f"Skipped (no meta):                         {rep.skipped_no_meta}")
        log.println(f"Skipped (bad meta/decode):                 {rep.skipped_bad_meta}")
        log.println(f"Skipped (extension mismatch):              {rep.skipped_extension_mismatch}")
        log.println(f"Rename collisions resolved:                {rep.rename_collisions_resolved}")
        log.println("")
        log.println(f"URL decoded names:                         {rep.url_decoded}")
        log.println(f"Decode failures:                           {rep.decode_failures}")
        log.println(f"Leftover %XX after decode:                 {rep.leftover_percent}")
        for ex in rep.leftover_examples:
            log.println(f"  - {ex}")
        log.println("")
        if args.overwrite:
            log.println(f"Source duplicates removed (overwrite mode): {rep.data_duplicates_removed}")
            if rep.data_duplicates_removed_examples:
                log.println("Examples (removed -> kept):")
                for r, k in rep.data_duplicates_removed_examples:
                    log.println(f"  - {r} -> {k}")

    finally:
        log.close()

    return 0


if __name__ == "__main__":
    raise SystemExit(main())

