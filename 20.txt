async fn run_batch_for_steps(
    &self,
    steps: &[(String, String, ProcessStep)],
) -> Result<()> {
    use std::collections::HashMap;

    // Grupowanie po exec_object_nm
    let mut grouped: HashMap<String, Vec<(String, String, ProcessStep)>> = HashMap::new();
    for (file_id, uri, step) in steps.iter().cloned() {
        grouped
            .entry(step.exec_object_nm.clone())
            .or_default()
            .push((file_id, uri, step));
    }

    for (exec_object_nm, items) in grouped {
        for chunk in items.chunks(self.cfg.batch_size) {
            // ================================================================
            // 1. Budujemy batch payload – KAŻDY ELEMENT MA SWÓJ trg_path !!!
            // ================================================================

            let batch_payload: Vec<_> = chunk
                .iter()
                .map(|(file_id, uri, step)| {
                    let tgt_path = step
                        .params
                        .get("trg_path")
                        .and_then(|v| v.as_str())
                        .unwrap_or("");

                    serde_json::json!({
                        "file_id": file_id,
                        "gcs_file_uri": uri,
                        "process_cd": step.process_cd,
                        "seq": step.exec_process_seq,
                        "trg_path": tgt_path,     // <-- tu jest klucz
                    })
                })
                .collect();

            let json_payload = serde_json::json!({
                "process_id": self.cfg.process_id,
                "dispatcher_run_id": self.dispatcher_run_id.to_string(),
                "batch": batch_payload
            });

            // ================================================================
            // 2. ENV VARS: tylko GLOBALNE (TGT_PATH usuwamy!)
            // ================================================================
            let envs = self.cfg.env_vars.clone();

            let exec_id = if let Some(cr) =
                self.backend.as_any().downcast_ref::<CloudRunBackend>()
            {
                cr.start_job(&exec_object_nm, &json_payload, &envs).await?
            } else {
                self.backend
                    .dispatch_job(&exec_object_nm, &json_payload)
                    .await?
            };

            // ================================================================
            // 3. Pollujemy (batch = sync mode)
            // ================================================================
            if let Some(cr) = self.backend.as_any().downcast_ref::<CloudRunBackend>() {
                cr.poll_until_done(
                    &exec_id,
                    Duration::from_secs(60 * 30),
                    Duration::from_secs(10),
                )
                .await?;
            } else {
                debug!(%exec_id, "Backend bez poll_until_done (batch mode)");
            }
        }
    }

    Ok(())
}


====

async fn run_dynamic_wfl_async_jobs(&self, files: Vec<FileProcess>) -> Result<()> {
        let db = self
            .db
            .as_ref()
            .cloned()
            .ok_or_else(|| anyhow::anyhow!("Brak połączenia z DB dla dynamicznego workflow"))?;

        let (tx_tasks, rx_tasks): (Sender<FileTask>, Receiver<FileTask>) = unbounded();
        let (tx_exec, rx_exec): (Sender<String>, Receiver<String>) = unbounded();

        let remaining_files = Arc::new(AtomicUsize::new(files.len()));
        let steps_cache: Arc<DashMap<String, VecDeque<ProcessStep>>> = Arc::new(DashMap::new());

        // =============================================================
        // 1. pierwszy krok per file z payloadu
        // =============================================================

        for f in &files {
            if let Some(first_step) = f.processes.iter().min_by_key(|s| s.exec_process_seq) {
                let task = FileTask {
                    file_id: f.file_id.clone(),
                    gcs_file_uri: f.gcs_file_uri.clone(),
                    step: first_step.clone(),
                };
                let _ = tx_tasks.send(task);
            } else {
                remaining_files.fetch_sub(1, Ordering::AcqRel);
            }
        }

        drop(files);

        // =============================================================
        // 2. Poller
        // =============================================================

        let rx_exec_for_poller = rx_exec.clone();
        let dispatcher_for_poller = self.clone_light();


        let poller_handle = tokio::spawn(async move {
            loop {
                let maybe_id = tokio::task::spawn_blocking({
                    let rx = rx_exec_for_poller.clone();
                    move || rx.recv()
                })
                    .await;

                let exec_id = match maybe_id {
                    Ok(Ok(id)) => id,
                    Ok(Err(_)) => break,
                    Err(e) => {
                        error!(?e, "DYNAMIC+ASYNC poller: JoinError");
                        break;
                    }
                };

                if let Some(cr) = dispatcher_for_poller
                    .backend
                    .as_any()
                    .downcast_ref::<CloudRunBackend>()
                {
                    let _ = cr
                        .poll_until_done(
                            &exec_id,
                            Duration::from_secs(1800),
                            Duration::from_secs(10),
                        )
                        .await;
                }
            }
        });


        // =============================================================
        // 3. Workers
        // =============================================================

        let mut worker_handles = Vec::new();

        for _ in 0..self.cfg.max_workers {
            let rx_tasks_clone = rx_tasks.clone();
            let tx_tasks_clone = tx_tasks.clone();
            let tx_exec_clone = tx_exec.clone();
            let dispatcher = self.clone_light();
            let remaining_files = Arc::clone(&remaining_files);
            let steps_cache = Arc::clone(&steps_cache);
            let db_clone = db.clone();

            let h = std::thread::spawn(move || {
                let rt = tokio::runtime::Runtime::new()
                    .expect("Tokio Runtime worker dynamic");

                while let Ok(task) = rx_tasks_clone.recv() {
                    let dispatcher_c = dispatcher.clone_light();
                    let db_for_call = db_clone.clone();

                    let res = rt.block_on(async {
                        dispatcher_c
                            .process_dynamic_task(
                                task,
                                &tx_tasks_clone,
                                &remaining_files,
                                &steps_cache,
                                &db_for_call,
                                &tx_exec_clone,
                            )
                            .await
                    });

                    if res.is_err() {
                        remaining_files.fetch_sub(1, Ordering::AcqRel);
                    }
                }
            });

            worker_handles.push(h);
        }

        drop(tx_tasks);

        for h in worker_handles {
            let _ = h.join();
        }

        drop(tx_exec);

        let _ = poller_handle.await;

        info!(
        remaining = remaining_files.load(Ordering::Acquire),
        "DYNAMIC+ASYNC zakończone"
    );

        Ok(())
    }


====

async fn process_dynamic_task(
    &self,
    task: FileTask,
    tx_tasks: &Sender<FileTask>,
    remaining_files: &Arc<AtomicUsize>,
    steps_cache: &Arc<DashMap<String, VecDeque<ProcessStep>>>,
    db: &Arc<Db>,
    tx_exec: &Sender<String>,
) -> Result<()> {
    let file_id = &task.file_id;
    let gcs_file_uri = &task.gcs_file_uri;
    let step = &task.step;

    // =============================================================
    // 1. ENV VARS per step (poprawione)
    // =============================================================

    let mut envs = self.cfg.env_vars.clone();
    let tgt_path = step
        .params
        .get("trg_path")
        .and_then(|v| v.as_str())
        .unwrap_or("");
    envs.insert("TGT_PATH".into(), tgt_path.to_string());

    let payload = serde_json::json!({
        "process_id": self.cfg.process_id,
        "dispatcher_run_id": self.dispatcher_run_id.to_string(),
        "file_id": file_id,
        "gcs_file_uri": gcs_file_uri,
        "process_cd": step.process_cd,
        "seq": step.exec_process_seq,
    });

    let job_name = &step.exec_object_nm;

    let exec_id = if let Some(cr) = self.backend.as_any().downcast_ref::<CloudRunBackend>() {
        cr.start_job(job_name, &payload, &envs).await?
    } else {
        self.backend.dispatch_job(job_name, &payload).await?
    };

    info!(
        %exec_id,
        %file_id,
        seq = step.exec_process_seq,
        obj = %step.exec_object_nm,
        "DYNAMIC+ASYNC: job uruchomiony"
    );

    // =============================================================
    // 2. wysyłamy exec ID do pollera
    // =============================================================

    if let Err(e) = tx_exec.send(exec_id) {
        error!(error = ?e, %file_id, "DYNAMIC+ASYNC nie udało się wysłać exec_id");
    }

    // =============================================================
    // 3. Ładujemy kolejne kroki z DB jeśli nie w cache
    // =============================================================

    if !steps_cache.contains_key(file_id) {
        let steps = db.get_steps_for_file(file_id).await?;
        if steps.is_empty() {
            let prev = remaining_files.fetch_sub(1, Ordering::AcqRel);
            info!(%file_id, prev_remaining = prev, "DYNAMIC+ASYNC: koniec pliku");
            return Ok(());
        }
        let mut sorted = steps;
        sorted.sort_by_key(|s| s.exec_process_seq);
        steps_cache.insert(file_id.clone(), sorted.into());
    }

    // =============================================================
    // 4. enqueue next
    // =============================================================

    if let Some(mut entry) = steps_cache.get_mut(file_id) {
        if let Some(next_step) = entry.pop_front() {
            let queue_empty = entry.is_empty();
            drop(entry);

            if queue_empty {
                steps_cache.remove(file_id);
            }

            let next_task = FileTask {
                file_id: file_id.clone(),
                gcs_file_uri: gcs_file_uri.clone(),
                step: next_step,
            };

            if let Err(e) = tx_tasks.send(next_task) {
                error!(error = ?e, "DYNAMIC+ASYNC: błąd wysłania next_step");
                remaining_files.fetch_sub(1, Ordering::AcqRel);
            }

            return Ok(());
        } else {
            drop(entry);
            steps_cache.remove(file_id);
        }
    }

    // =============================================================
    // 5. koniec pliku
    // =============================================================

    remaining_files.fetch_sub(1, Ordering::AcqRel);
    Ok(())
}

====

async fn run_static_wfl_async_jobs(&self, files: Vec<FileProcess>) -> Result<()> {
        let (tx_tasks, rx_tasks): (Sender<FileTask>, Receiver<FileTask>) = unbounded();
        let (tx_exec, rx_exec): (Sender<String>, Receiver<String>) = unbounded();

        let remaining_files = Arc::new(AtomicUsize::new(files.len()));
        let steps_cache: Arc<DashMap<String, VecDeque<ProcessStep>>> = Arc::new(DashMap::new());

        // =============================================================
        // 1. pierwszy krok dla każdego pliku
        // =============================================================

        for f in &files {
            if f.processes.is_empty() {
                remaining_files.fetch_sub(1, Ordering::AcqRel);
                continue;
            }

            let mut steps = f.processes.clone();
            steps.sort_by_key(|s| s.exec_process_seq);

            let first_step = steps.remove(0);
            steps_cache.insert(f.file_id.clone(), steps.into());

            tx_tasks.send(FileTask {
                file_id: f.file_id.clone(),
                gcs_file_uri: f.gcs_file_uri.clone(),
                step: first_step,
            })?;
        }

        drop(files);

        // =============================================================
        // 2. Pojedynczy poller
        // =============================================================

        // 2. ASYNC POLLER (tokio::spawn)
        let rx_exec_for_poller = rx_exec.clone();
        let dispatcher_for_poller = self.clone_light();

        let poller_handle = tokio::spawn(async move {
            loop {
                let maybe_id = tokio::task::spawn_blocking({
                    let rx = rx_exec_for_poller.clone();
                    move || rx.recv()
                })
                    .await;

                let exec_id = match maybe_id {
                    Ok(Ok(id)) => id,
                    Ok(Err(_)) => {
                        info!("STATIC+ASYNC poller: kanał exec_id zamknięty — kończę");
                        break;
                    }
                    Err(e) => {
                        error!(?e, "STATIC+ASYNC poller: JoinError");
                        break;
                    }
                };

                if let Some(cr) = dispatcher_for_poller
                    .backend
                    .as_any()
                    .downcast_ref::<CloudRunBackend>()
                {
                    if let Err(e) = cr
                        .poll_until_done(
                            &exec_id,
                            Duration::from_secs(1800),
                            Duration::from_secs(10),
                        )
                        .await
                    {
                        error!(error=?e, %exec_id, "STATIC+ASYNC poller: błąd poll");
                    }
                }
            }
        });


        // =============================================================
        // 3. Worker pool
        // =============================================================

        let workers = self.cfg.max_workers;
        let mut worker_handles = Vec::new();

        for _ in 0..workers {
            let rx_tasks_clone = rx_tasks.clone();
            let tx_tasks_clone = tx_tasks.clone();
            let tx_exec_clone = tx_exec.clone();
            let dispatcher = self.clone_light();
            let remaining_files = Arc::clone(&remaining_files);
            let steps_cache = Arc::clone(&steps_cache);

            let h = std::thread::spawn(move || {
                let rt = tokio::runtime::Runtime::new()
                    .expect("Tokio Runtime worker error");

                while let Ok(task) = rx_tasks_clone.recv() {
                    let dispatcher_c = dispatcher.clone_light();

                    let res = rt.block_on(async {
                        dispatcher_c
                            .process_static_task(
                                task,
                                &tx_tasks_clone,
                                &remaining_files,
                                &steps_cache,
                                &tx_exec_clone,
                            )
                            .await
                    });

                    if let Err(e) = res {
                        error!(error = ?e, "STATIC+ASYNC worker error");
                    }
                }
            });

            worker_handles.push(h);
        }

        drop(tx_tasks);

        for h in worker_handles {
            let _ = h.join();
        }

        drop(tx_exec);

        let _ = poller_handle.await;

        info!(remaining = remaining_files.load(Ordering::Acquire), "STATIC+ASYNC zakończone");
        Ok(())
    }



====


async fn process_static_task(
    &self,
    task: FileTask,
    tx_tasks: &Sender<FileTask>,
    remaining_files: &Arc<AtomicUsize>,
    steps_cache: &Arc<DashMap<String, VecDeque<ProcessStep>>>,
    tx_exec: &Sender<String>,
) -> Result<()> {
    let file_id = &task.file_id;
    let gcs_file_uri = &task.gcs_file_uri;
    let step = &task.step;

    // =============================================================
    // 1. start_job z kompletami ENV VARS (poprawione)
    // =============================================================

    let mut envs = self.cfg.env_vars.clone();
    let tgt_path = step
        .params
        .get("trg_path")
        .and_then(|v| v.as_str())
        .unwrap_or("");

    envs.insert("TGT_PATH".into(), tgt_path.to_string());

    let payload = serde_json::json!({
        "process_id": self.cfg.process_id,
        "dispatcher_run_id": self.dispatcher_run_id.to_string(),
        "file_id": file_id,
        "gcs_file_uri": gcs_file_uri,
        "process_cd": step.process_cd,
        "seq": step.exec_process_seq,
    });

    let job_name = &step.exec_object_nm;

    let exec_id = if let Some(cr) = self.backend.as_any().downcast_ref::<CloudRunBackend>() {
        cr.start_job(job_name, &payload, &envs).await?
    } else {
        self.backend.dispatch_job(job_name, &payload).await?
    };

    info!(
        %exec_id,
        %file_id,
        seq = step.exec_process_seq,
        obj = %step.exec_object_nm,
        "STATIC+ASYNC: job uruchomiony"
    );

    // =============================================================
    // 2. przekazujemy exec_id do centralnego pollera
    // =============================================================

    if let Err(e) = tx_exec.send(exec_id) {
        error!(error = ?e, %file_id, "STATIC+ASYNC: błąd wysyłania exec_id do pollera");
    }

    // =============================================================
    // 3. enqueue następnego kroku
    // =============================================================

    if let Some(mut entry) = steps_cache.get_mut(file_id) {
        if let Some(next_step) = entry.pop_front() {
            let queue_empty = entry.is_empty();
            drop(entry);
            if queue_empty {
                steps_cache.remove(file_id);
            }

            let new_task = FileTask {
                file_id: file_id.clone(),
                gcs_file_uri: gcs_file_uri.clone(),
                step: next_step,
            };

            if let Err(e) = tx_tasks.send(new_task) {
                error!(error = ?e, %file_id, "STATIC+ASYNC: błąd wysyłania kolejnego kroku");
                remaining_files.fetch_sub(1, Ordering::AcqRel);
            }

            return Ok(());
        } else {
            drop(entry);
            steps_cache.remove(file_id);
        }
    }

    // =============================================================
    // 4. brak kolejnych kroków → plik zakończony
    // =============================================================

    remaining_files.fetch_sub(1, Ordering::AcqRel);
    Ok(())
}
