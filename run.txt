/// Ogólne wykonanie batcha kroków (file_id, uri, step) z uwzględnieniem grouping.
    async fn run_batch_for_steps(
        &self,
        steps: &[(String, String, ProcessStep)],
    ) -> Result<()> {
        match self.cfg.grouping {
            Grouping::PerImage => {
                use std::collections::HashMap;
                let mut grouped: HashMap<String, Vec<(String, String, ProcessStep)>> =
                    HashMap::new();
                for (file_id, uri, step) in steps.iter().cloned() {
                    grouped
                        .entry(step.exec_object_nm.clone())
                        .or_default()
                        .push((file_id, uri, step));
                }

                for (exec_object_nm, items) in grouped {
                    for chunk in items.chunks(self.cfg.batch_size) {
                        let batch_payload: Vec<_> = chunk
                            .iter()
                            .map(|(file_id, uri, step)| {
                                serde_json::json!({
                                    "file_id": file_id,
                                    "gcs_file_uri": uri,
                                    "process_cd": step.process_cd,
                                    "seq": step.exec_process_seq,
                                })
                            })
                            .collect();

                        let json_payload = serde_json::json!({
                            "process_id": self.cfg.process_id,
                            "dispatcher_run_id": self.dispatcher_run_id.to_string(),
                            "batch": batch_payload
                        });

                        let exec_id = if let Some(cr) = self
                            .backend
                            .as_any()
                            .downcast_ref::<CloudRunBackend>()
                        {
                            cr.start_job(&exec_object_nm, &json_payload).await?
                        } else {
                            self.backend
                                .dispatch_job(&exec_object_nm, &json_payload)
                                .await?
                        };

                        if let Some(cr) = self.backend.as_any().downcast_ref::<CloudRunBackend>() {
                            cr.poll_until_done(
                                &exec_id,
                                Duration::from_secs(60 * 30),
                                Duration::from_secs(10),
                            )
                            .await?;
                        } else {
                            debug!(%exec_id, "Backend bez poll_until_done (PerImage)");
                        }
                    }
                }
            }
            Grouping::PerFile => {
                // Każdy plik / step jako osobny job, sekwencyjnie
                for (file_id, uri, step) in steps {
                    let payload = serde_json::json!({
                        "process_id": self.cfg.process_id,
                        "dispatcher_run_id": self.dispatcher_run_id.to_string(),
                        "file_id": file_id,
                        "gcs_file_uri": uri,
                        "process_cd": step.process_cd,
                        "seq": step.exec_process_seq,
                    });

                    let job_name = &step.exec_object_nm;

                    let exec_id = if let Some(cr) = self
                        .backend
                        .as_any()
                        .downcast_ref::<CloudRunBackend>()
                    {
                        cr.start_job(job_name, &payload).await?
                    } else {
                        self.backend.dispatch_job(job_name, &payload).await?
                    };

                    if let Some(cr) = self.backend.as_any().downcast_ref::<CloudRunBackend>() {
                        cr.poll_until_done(
                            &exec_id,
                            Duration::from_secs(60 * 30),
                            Duration::from_secs(10),
                        )
                        .await?;
                    } else {
                        debug!(%exec_id, "Backend bez poll_until_done (PerFile)");
                    }
                }
            }
        }

        Ok(())
    }

    // ---------------------------------------------------------------------------------
    // STATIC + BATCH
    // ---------------------------------------------------------------------------------

    async fn run_static_wfl_batch_jobs(&self, files: &[FileProcess]) -> Result<()> {
        let seqs = Self::collect_sorted_seqs(files);

        for seq in seqs {
            // zbierz wszystkie kroki o danym seq
            let mut steps_vec: Vec<(String, String, ProcessStep)> = Vec::new();
            for f in files {
                for p in &f.processes {
                    if p.exec_process_seq == seq {
                        steps_vec.push((f.file_id.clone(), f.gcs_file_uri.clone(), p.clone()));
                    }
                }
            }

            if steps_vec.is_empty() {
                continue;
            }

            info!(seq, count = steps_vec.len(), "STATIC+BATCH: uruchamiam batch dla seq");
            self.run_batch_for_steps(&steps_vec).await?;
        }

        Ok(())
    }

    // ---------------------------------------------------------------------------------
    // STATIC + ASYNC
    // ---------------------------------------------------------------------------------

    async fn run_static_wfl_async_jobs(&self, files: Vec<FileProcess>) -> Result<()> {
        let (tx, rx): (Sender<FileTask>, Receiver<FileTask>) = unbounded();

        for f in &files {
            for step in &f.processes {
                let task = FileTask {
                    file_id: f.file_id.clone(),
                    gcs_file_uri: f.gcs_file_uri.clone(),
                    step: step.clone(),
                };
                if let Err(e) = tx.send(task) {
                    error!(error = ?e, "Błąd wysyłania zadania do kolejki (STATIC+ASYNC)");
                }
            }
        }

        drop(files);

        // spawn workerów
        let workers = self.cfg.max_workers;
        info!(workers, "STATIC+ASYNC: uruchamiam workerów");

        let mut handles = Vec::new();
        let rt_handle = tokio::runtime::Handle::current();

        for _ in 0..workers {
            let rx_clone = rx.clone();
            let dispatcher = self.clone_light();

            let h = std::thread::spawn(move || {
                while let Ok(task) = rx_clone.recv() {
                    let result = rt_handle.block_on(async {
                        dispatcher
                            .dispatch_single_static(&task.file_id, &task.gcs_file_uri, &task.step)
                            .await
                    });
                    if let Err(e) = result {
                        error!(error = ?e, file_id = %task.file_id, "Błąd w workerze STATIC+ASYNC");
                    }
                }
            });
            handles.push(h);
        }

        // zamykamy główny sender – workerzy zakończą się kiedy kolejka się wyczerpie
        drop(tx);

        for h in handles {
            if let Err(e) = h.join() {
                error!("Join worker STATIC+ASYNC nie powiódł się: {:?}", e);
            }
        }

        info!("STATIC+ASYNC: wszystkie zadania zakończone");
        Ok(())
    }

    async fn dispatch_single_static(
        &self,
        file_id: &str,
        gcs_file_uri: &str,
        step: &ProcessStep,
    ) -> Result<()> {
        let payload = serde_json::json!({
            "process_id": self.cfg.process_id,
            "dispatcher_run_id": self.dispatcher_run_id.to_string(),
            "file_id": file_id,
            "gcs_file_uri": gcs_file_uri,
            "process_cd": step.process_cd,
            "seq": step.exec_process_seq,
        });

        let job_name = &step.exec_object_nm;

        let exec_id = if let Some(cr) = self
            .backend
            .as_any()
            .downcast_ref::<CloudRunBackend>()
        {
            cr.start_job(job_name, &payload).await?
        } else {
            self.backend.dispatch_job(job_name, &payload).await?
        };

        info!(
            %exec_id,
            %file_id,
            seq = step.exec_process_seq,
            obj = %step.exec_object_nm,
            "STATIC+ASYNC: job uruchomiony (single)"
        );

        if let Some(cr) = self.backend.as_any().downcast_ref::<CloudRunBackend>() {
            cr.poll_until_done(
                &exec_id,
                Duration::from_secs(60 * 30),
                Duration::from_secs(10),
            )
            .await?;
        }

        Ok(())
    }

    // ---------------------------------------------------------------------------------
    // DYNAMIC + BATCH
    // ---------------------------------------------------------------------------------

    async fn run_dynamic_wfl_batch_jobs(&self, files: Vec<FileProcess>) -> Result<()> {
        let db = self
            .db
            .as_ref()
            .cloned()
            .ok_or_else(|| anyhow!("Brak połączenia z DB dla dynamicznego workflow"))?;

        // 1. Pierwszy krok z payloadu (SEQ minimalne dla danego pliku)
        let mut first_steps: Vec<(String, String, ProcessStep)> = Vec::new();
        for f in &files {
            if let Some(first_step) = f.processes.iter().min_by_key(|s| s.exec_process_seq) {
                first_steps.push((
                    f.file_id.clone(),
                    f.gcs_file_uri.clone(),
                    first_step.clone(),
                ));
            } else {
                warn!(
                    file_id = %f.file_id,
                    "DYNAMIC+BATCH: plik bez pierwszego kroku w payloadzie – pomijam"
                );
            }
        }

        if first_steps.is_empty() {
            warn!("DYNAMIC+BATCH: brak pierwszych kroków – nic do zrobienia");
            return Ok(());
        }

        info!(
            count = first_steps.len(),
            "DYNAMIC+BATCH: uruchamiam pierwszy batch (payload)"
        );
        self.run_batch_for_steps(&first_steps).await?;

        // 2. Wczytujemy z DB wszystkie przyszłe kroki (2,3,...) dla każdego pliku
        use std::collections::HashMap;
        let mut steps_map: HashMap<String, VecDeque<ProcessStep>> = HashMap::new();
        let mut last_seq: HashMap<String, u32> = HashMap::new();

        for (file_id, _, step) in &first_steps {
            last_seq.insert(file_id.clone(), step.exec_process_seq);

            let steps = db.get_steps_for_file(file_id).await?;
            if steps.is_empty() {
                debug!(%file_id, "DYNAMIC+BATCH: brak kolejnych kroków w DB");
            } else {
                let mut v: Vec<ProcessStep> = steps;
                v.sort_by_key(|s| s.exec_process_seq);
                steps_map.insert(file_id.clone(), v.into());
            }
        }

        // 3. Kolejne batche po seq – aż brak dalszych kroków
        loop {
            let mut batch_steps: Vec<(String, String, ProcessStep)> = Vec::new();

            for f in &files {
                let file_id = &f.file_id;
                let uri = &f.gcs_file_uri;

                let last = match last_seq.get(file_id) {
                    Some(v) => *v,
                    None => continue,
                };

                if let Some(queue) = steps_map.get_mut(file_id) {
                    // bierzemy kolejny krok z kolejki, jeśli istnieje
                    if let Some(next_step) = queue.pop_front() {
                        if next_step.exec_process_seq <= last {
                            warn!(
                                %file_id,
                                prev_seq = last,
                                next_seq = next_step.exec_process_seq,
                                "DYNAMIC+BATCH: krok z DB ma seq <= poprzedni – pomijam"
                            );
                        } else {
                            batch_steps.push((file_id.clone(), uri.clone(), next_step.clone()));
                            last_seq.insert(file_id.clone(), next_step.exec_process_seq);
                        }
                    }

                    if queue.is_empty() {
                        steps_map.remove(file_id);
                    }
                }
            }

            if batch_steps.is_empty() {
                info!("DYNAMIC+BATCH: brak kolejnych kroków – pipeline zakończony");
                break;
            }

            info!(
                count = batch_steps.len(),
                "DYNAMIC+BATCH: uruchamiam kolejny batch"
            );
            self.run_batch_for_steps(&batch_steps).await?;
        }

        Ok(())
    }

    // ---------------------------------------------------------------------------------
    // DYNAMIC + ASYNC (independent per file)
// ---------------------------------------------------------------------------------

    async fn run_dynamic_wfl_async_jobs(&self, files: Vec<FileProcess>) -> Result<()> {
        let db = self
            .db
            .as_ref()
            .cloned()
            .ok_or_else(|| anyhow!("Brak połączenia z DB dla dynamicznego workflow"))?;

        let (tx, rx): (Sender<FileTask>, Receiver<FileTask>) = unbounded();
        let remaining_files = Arc::new(AtomicUsize::new(files.len()));
        let steps_cache: Arc<DashMap<String, VecDeque<ProcessStep>>> = Arc::new(DashMap::new());

        // 1. wrzucamy pierwszy krok z payloadu dla każdego pliku
        for f in &files {
            if let Some(first_step) = f.processes.iter().min_by_key(|s| s.exec_process_seq) {
                let task = FileTask {
                    file_id: f.file_id.clone(),
                    gcs_file_uri: f.gcs_file_uri.clone(),
                    step: first_step.clone(),
                };
                if let Err(e) = tx.send(task) {
                    error!(error = ?e, "Błąd wysyłania początkowego zadania (DYNAMIC+ASYNC)");
                    remaining_files.fetch_sub(1, Ordering::AcqRel);
                }
            } else {
                warn!(
                    file_id = %f.file_id,
                    "DYNAMIC+ASYNC: plik bez pierwszego kroku w payloadzie – traktuję jako zakończony"
                );
                remaining_files.fetch_sub(1, Ordering::AcqRel);
            }
        }

        drop(files);

        let workers = self.cfg.max_workers;
        info!(workers, "DYNAMIC+ASYNC: uruchamiam workerów");

        let mut handles = Vec::new();
        let rt_handle = tokio::runtime::Handle::current();

        for _ in 0..workers {
            let rx_clone = rx.clone();
            let tx_clone = tx.clone();
            let dispatcher = self.clone_light();
            let remaining_files = Arc::clone(&remaining_files);
            let steps_cache = Arc::clone(&steps_cache);
            let db_clone = db.clone();

            let h = std::thread::spawn(move || {
                while let Ok(task) = rx_clone.recv() {
                    let result = rt_handle.block_on(async {
                        dispatcher
                            .process_dynamic_task(
                                task,
                                &tx_clone,
                                &remaining_files,
                                &steps_cache,
                                &db_clone,
                            )
                            .await
                    });

                    if let Err(e) = result {
                        error!(error = ?e, "Błąd w workerze DYNAMIC+ASYNC");
                    }
                }
            });
            handles.push(h);
        }

        // główny sender już niepotrzebny
        drop(tx);

        // czekamy aż workerzy się zakończą
        for h in handles {
            if let Err(e) = h.join() {
                error!("Join worker DYNAMIC+ASYNC nie powiódł się: {:?}", e);
            }
        }

        info!(
            remaining = remaining_files.load(Ordering::Acquire),
            "DYNAMIC+ASYNC: workerzy zakończeni"
        );

        Ok(())
    }

    async fn process_dynamic_task(
        &self,
        task: FileTask,
        tx: &Sender<FileTask>,
        remaining_files: &Arc<AtomicUsize>,
        steps_cache: &Arc<DashMap<String, VecDeque<ProcessStep>>>,
        db: &Arc<Db>,
    ) -> Result<()> {
        let file_id = &task.file_id;
        let gcs_file_uri = &task.gcs_file_uri;
        let step = &task.step;

        // 1. uruchamiamy job dla bieżącego kroku
        let payload = serde_json::json!({
            "process_id": self.cfg.process_id,
            "dispatcher_run_id": self.dispatcher_run_id.to_string(),
            "file_id": file_id,
            "gcs_file_uri": gcs_file_uri,
            "process_cd": step.process_cd,
            "seq": step.exec_process_seq,
        });

        let job_name = &step.exec_object_nm;

        let exec_id = if let Some(cr) = self
            .backend
            .as_any()
            .downcast_ref::<CloudRunBackend>()
        {
            cr.start_job(job_name, &payload).await?
        } else {
            self.backend.dispatch_job(job_name, &payload).await?
        };

        info!(
            %exec_id,
            %file_id,
            seq = step.exec_process_seq,
            obj = %step.exec_object_nm,
            "DYNAMIC+ASYNC: job uruchomiony"
        );

        if let Some(cr) = self.backend.as_any().downcast_ref::<CloudRunBackend>() {
            cr.poll_until_done(
                &exec_id,
                Duration::from_secs(60 * 30),
                Duration::from_secs(10),
            )
            .await?;
        }

        // 2. Po zakończeniu kroku – pobieramy (jeśli jeszcze nie ma) listę kolejnych kroków z DB
        if !steps_cache.contains_key(file_id) {
            let steps = db.get_steps_for_file(file_id).await?;
            if steps.is_empty() {
                debug!(%file_id, "DYNAMIC+ASYNC: brak kolejnych kroków w DB");
                let prev = remaining_files.fetch_sub(1, Ordering::AcqRel);
                info!(
                    %file_id,
                    prev_remaining = prev,
                    "DYNAMIC+ASYNC: plik zakończony (brak kroków)"
                );
                return Ok(());
            } else {
                let mut v: Vec<ProcessStep> = steps;
                v.sort_by_key(|s| s.exec_process_seq);
                steps_cache.insert(file_id.clone(), v.into());
            }
        }

        if let Some(mut entry) = steps_cache.get_mut(file_id) {
            if let Some(next_step) = entry.pop_front() {
                let queue_empty = entry.is_empty();
                drop(entry);
                if queue_empty {
                    steps_cache.remove(file_id);
                }

                let new_task = FileTask {
                    file_id: file_id.clone(),
                    gcs_file_uri: gcs_file_uri.clone(),
                    step: next_step,
                };
                if let Err(e) = tx.send(new_task) {
                    error!(
                        error = ?e,
                        %file_id,
                        "DYNAMIC+ASYNC: błąd wysyłania kolejnego kroku do kolejki"
                    );
                    let prev = remaining_files.fetch_sub(1, Ordering::AcqRel);
                    info!(
                        %file_id,
                        prev_remaining = prev,
                        "DYNAMIC+ASYNC: plik zakończony z błędem send()"
                    );
                }
            } else {
                drop(entry);
                steps_cache.remove(file_id);
                let prev = remaining_files.fetch_sub(1, Ordering::AcqRel);
                info!(
                    %file_id,
                    prev_remaining = prev,
                    "DYNAMIC+ASYNC: plik zakończony (kolejka kroków pusta)"
                );
            }
        } else {
            let prev = remaining_files.fetch_sub(1, Ordering::AcqRel);
            info!(
                %file_id,
                prev_remaining = prev,
                "DYNAMIC+ASYNC: plik zakończony (brak entry w cache)"
            );
        }

        Ok(())
    }
