//! UDR Dispatcher
//! - przyjmuje payload plików,
//! - tworzy sesję (dispatcher_run_id),
//! - wznawia przerwane zadania (REDISPATCHED),
//! - etap A: odpala meta-extractor i czeka na META_EXTRACTED,
//! - etap B: grupuje po rozszerzeniu → dispatch do odpowiednich Cloud Run Jobs,
//! - monitoruje statusy (LISTEN/NOTIFY + okresowy polling + „pingi”),
//! - kończy po FINISHED lub pozostawia retry na kolejne instancje (FAILED/TIMED_OUT),
//! - ma tryb --dry-run (bez DB/Cloud Run).
//!
mod backend;
mod cloud_run;

use backend::ProcessorBackend;
use cloud_run::CloudRunBackend;

use postgres_types::FromSql;
use tracing_subscriber::prelude::*;

use anyhow::{Context, Result};
use chrono::{DateTime, Utc};
use clap::{ArgAction, Parser, ValueHint};
use dashmap::DashSet;
use futures::{stream, StreamExt, TryStreamExt};
use serde::{Deserialize, Serialize};
use std::{collections::{BTreeMap, HashMap}, path::PathBuf, sync::Arc, time::Duration};
use tokio::{process::Command, sync::mpsc::{unbounded_channel, UnboundedReceiver, UnboundedSender}};
use tokio_postgres::{Client, NoTls, Row};
use tokio_stream::wrappers::IntervalStream;
use tracing::{debug, error, info, warn};
use uuid::Uuid;

// ======================== CLI & CONFIG ========================

#[derive(Parser, Debug, Clone)]
#[command(name = "udr-dispatcher", version, about = "Universal Data Repo Dispatcher (PDF/MD/DOCX)")]
struct Cli {
    /// Global process/session id nadawany przez orkiestrator
    #[arg(long, value_parser, value_hint = ValueHint::Other)]
    process_id: String,

    /// GCP project id (przekazywany do jobów)
    #[arg(long)]
    gcp_project: Option<String>,

    /// DSN do Postgresa lub osobne parametry (DSN)
    /// np. "host=127.0.0.1 user=udr password=*** dbname=udr port=5432"
    #[arg(long, value_hint = ValueHint::Other)]
    pg_dsn: Option<String>,

    /// Alternatywnie: host
    #[arg(long)]
    pg_host: Option<String>,
    /// Alternatywnie: port
    #[arg(long, default_value_t = 5432)]
    pg_port: u16,
    /// Alternatywnie: user
    #[arg(long)]
    pg_user: Option<String>,
    /// Alternatywnie: password
    #[arg(long)]
    pg_password: Option<String>,
    /// Alternatywnie: dbname
    #[arg(long)]
    pg_dbname: Option<String>,

    /// JSON listy plików do przetworzenia – bez spacji OK
    /// [{"file_path":"...","file_id":"123","export_path":"..."}]
    #[arg(long)]
    payload: Option<String>,

    /// Plik z payloadem (alternatywa wobec --payload)
    #[arg(long, value_hint = ValueHint::FilePath)]
    payload_file: Option<PathBuf>,

    /// Gdzie finalnie zapisać meta.json (folder lub pełna ścieżka pliku)
    #[arg(long, value_hint = ValueHint::AnyPath)]
    export_dir: Option<PathBuf>,

    /// Ile maksymalnie powtórzeń (ponowień) na plik
    #[arg(long, default_value_t = 3)]
    max_retries: u32,

    /// Dry run – bez łączenia z DB i bez wywołań Cloud Run
    #[arg(long, action=ArgAction::SetTrue)]
    dry_run: bool,

    /// Co ile sekund robić „polling” statusów, jeśli nie ma NOTIFY
    #[arg(long, default_value_t = 30)]
    poll_interval_secs: u64,

    /// Co ile sekund pingować joby (healthcheck) – 0 = wyłączone
    #[arg(long, default_value_t = 0)]
    ping_interval_secs: u64,

    /// Nazwa kanału LISTEN/NOTIFY
    #[arg(long, default_value = "udr_job_status")]
    notify_channel: String,

    /// Nazwy jobów (Cloud Run) – meta extractor oraz mapowanie rozszerzeń → job
    #[arg(long, default_value = "udr-meta-extractor")]
    meta_job: String,

    /// Mapowanie rozszerzeń na joby w formacie CSV: "pdf:pdf-processing,md:md-processing,docx:docx-processing"
    #[arg(long, default_value = "pdf:pdf-processing,md:md-processing,docx:docx-processing")]
    ext_jobs: String,
}

#[derive(Debug, Clone)]
struct AppConfig {
    process_id: String,
    gcp_project: Option<String>,
    pg_dsn: Option<String>,
    pg_host: Option<String>,
    pg_port: u16,
    pg_user: Option<String>,
    pg_password: Option<String>,
    pg_dbname: Option<String>,
    max_retries: u32,
    dry_run: bool,
    poll_interval: Duration,
    ping_interval: Option<Duration>,
    notify_channel: String,
    meta_job: String,
    ext_jobs: HashMap<String, String>,
    export_dir: Option<PathBuf>,
}

impl From<Cli> for AppConfig {
    fn from(c: Cli) -> Self {
        let ext_jobs = parse_ext_jobs(&c.ext_jobs);
        let ping_interval = if c.ping_interval_secs == 0 {
            None
        } else {
            Some(Duration::from_secs(c.ping_interval_secs))
        };
        Self {
            process_id: c.process_id,
            gcp_project: c.gcp_project,
            pg_dsn: c.pg_dsn,
            pg_host: c.pg_host,
            pg_port: c.pg_port,
            pg_user: c.pg_user,
            pg_password: c.pg_password,
            pg_dbname: c.pg_dbname,
            max_retries: c.max_retries,
            dry_run: c.dry_run,
            poll_interval: Duration::from_secs(c.poll_interval_secs),
            ping_interval,
            notify_channel: c.notify_channel,
            meta_job: c.meta_job,
            ext_jobs,
            export_dir: c.export_dir,
        }
    }
}

fn parse_ext_jobs(s: &str) -> HashMap<String, String> {
    s.split(',')
        .filter_map(|kv| {
            let mut it = kv.split(':');
            let k = it.next()?.trim().to_lowercase();
            let v = it.next()?.trim().to_string();
            Some((k, v))
        })
        .collect()
}

// ======================== DOMAIN ========================

#[derive(Debug, Clone, Serialize, Deserialize)]
struct FileTask {
    file_path: String,
    file_id: String,
    export_path: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct Payload {
    tasks: Vec<FileTask>,
}

#[derive(Debug, Clone, Copy, PartialEq, Eq)]
enum Status {
    /// Wysłane do meta-extractor
    MetaDispatched,
    /// Meta zakończone
    MetaExtracted,
    /// Wysłane do przetwarzania właściwego
    Dispatched,
    /// Ukończone przetwarzanie właściwe
    Completed,
    /// Zbiorczy final (meta.json zapisane)
    Finished,
    /// Błąd przetwarzania
    Failed,
    /// Przekroczony czas
    TimedOut,
    /// Plik podejmowany ponownie
    Redispatched,
    /// Inny/nieznany
    Other,
}

impl Status {
    fn from_str_loose(s: &str) -> Self {
        match s.to_uppercase().as_str() {
            "META_DISPATCHED" => Status::MetaDispatched,
            "META_EXTRACTED" => Status::MetaExtracted,
            "DISPATCHED" => Status::Dispatched,
            "COMPLETED" => Status::Completed,
            "FINISHED" => Status::Finished,
            "FAILED" => Status::Failed,
            "TIMED_OUT" => Status::TimedOut,
            "REDISPATCHED" => Status::Redispatched,
            _ => Status::Other,
        }
    }
    fn as_str(&self) -> &'static str {
        match self {
            Status::MetaDispatched => "META_DISPATCHED",
            Status::MetaExtracted => "META_EXTRACTED",
            Status::Dispatched => "DISPATCHED",
            Status::Completed => "COMPLETED",
            Status::Finished => "FINISHED",
            Status::Failed => "FAILED",
            Status::TimedOut => "TIMED_OUT",
            Status::Redispatched => "REDISPATCHED",
            Status::Other => "OTHER",
        }
    }
}

// ======================== BACKENDS (Cloud Run Jobs / Pingi) ========================

// #[async_trait::async_trait]
// trait ProcessorBackend: Send + Sync {
//     /// Dispatchuje job o podanej nazwie z JSON payloadem.
//     async fn dispatch_job(&self, job_name: &str, json_payload: &serde_json::Value) -> Result<String>;
//     /// Opcjonalny healthcheck jobu – np. GET na /health końcówki (jeśli to Services).
//     async fn ping(&self, job_name: &str) -> Result<()>;
// }

struct DryRunBackend;
#[async_trait::async_trait]
impl ProcessorBackend for DryRunBackend {
    async fn dispatch_job(&self, job_name: &str, json_payload: &serde_json::Value) -> Result<String> {
        info!(%job_name, payload = %json_payload, "DRY-RUN dispatch");
        Ok(format!("dryrun-{}", job_name))
    }
    async fn ping(&self, job_name: &str) -> Result<()> {
        info!(%job_name, "DRY-RUN ping ok");
        Ok(())
    }
    fn as_any(&self) -> &dyn std::any::Any {
        self
    }

}

/// Przykładowy backend „shell” – woła zewnętrzne polecenie (np. gcloud run jobs execute ...).
struct ShellBackend {
    project: Option<String>,
}
#[async_trait::async_trait]
impl ProcessorBackend for ShellBackend {
    async fn dispatch_job(&self, job_name: &str, json_payload: &serde_json::Value) -> Result<String> {
        let payload_str = serde_json::to_string(json_payload)?;
        let mut cmd = Command::new("bash");
        // przyklad wywoalania
        let script = format!(
            r#"echo "Dispatching {job} with payload: {payload}" && echo "{job}-run-$(date +%s)""#,
            job = job_name,
            payload = payload_str.replace('"', r#"\""#)
        );
        cmd.arg("-lc").arg(script);
        let out = cmd.output().await.context("shell dispatch failed")?;
        if !out.status.success() {
            anyhow::bail!("shell exit != 0: {}", String::from_utf8_lossy(&out.stderr));
        }
        let id = String::from_utf8_lossy(&out.stdout).trim().to_string();
        info!(%job_name, %id, "Shell dispatch ok");
        Ok(id)
    }

    async fn ping(&self, job_name: &str) -> Result<()> {
        // „ping” udajemy – w realu zrób HTTP/REST do swojej sondy
        info!(%job_name, "Shell ping ok");
        Ok(())
    }

    fn as_any(&self) -> &dyn std::any::Any {
        self
    }

}

// ======================== DB LAYER (prosty, bez ciężkich ORM) ========================

struct Db {
    client: Client,
}

impl Db {
    async fn connect(cfg: &AppConfig) -> Result<Self> {
        let dsn = if let Some(dsn) = &cfg.pg_dsn {
            dsn.clone()
        } else {
            // Buduje dsn z pojedynczych:
            format!(
                "host={} port={} user={} password={} dbname={}",
                cfg.pg_host.clone().unwrap_or_else(|| "127.0.0.1".into()),
                cfg.pg_port,
                cfg.pg_user.clone().unwrap_or_else(|| "postgres".into()),
                cfg.pg_password.clone().unwrap_or_default(),
                cfg.pg_dbname.clone().unwrap_or_else(|| "postgres".into())
            )
        };
        info!(%dsn, "Łączenie z Postgres (NoTls)...");
        let (client, conn) = tokio_postgres::connect(&dsn, NoTls).await?;
        // uruchamiam task obsługujący połączenie
        tokio::spawn(async move {
            if let Err(e) = conn.await {
                error!(error = ?e, "Błąd tła połączenia z Postgres");
            }
        });
        Ok(Self { client })
    }

    async fn init_session(&self, dispatcher_run_id: Uuid, process_id: &str) -> Result<()> {
        // jaki model? trzeba dostosowac do modelu tabeli „dispatcher_session”
        let q = r#"
            insert into dispatcher_session(dispatcher_run_id, process_id, start_ts)
            values ($1, $2, now())
            on conflict (dispatcher_run_id) do nothing
        "#;
        self.client
            .execute(q, &[&dispatcher_run_id, &process_id])
            .await
            .context("insert dispatcher_session")?;
        Ok(())
    }

    // async fn mark_redispached_required_and_collect(&self, process_id: &str) -> Result<Vec<String>> {
    //     // Znajdź pliki do wznowienia (FAILED/TIMED_OUT), ustaw REDISPATCHED
    //     let q_select = r#"
    //         select file_id from files
    //         where process_id = $1
    //           and status in ('FAILED','TIMED_OUT')
    //     "#;
    //     let rows = self.client.query(q_select, &[&process_id]).await?;
    //     let needs: Vec<String> = rows.iter().map(|r| r.get::<_, String>(0)).collect();
    //
    //     if !needs.is_empty() {
    //         let q_update = r#"
    //             update files set status = 'REDISPATCHED', retries = coalesce(retries,0) + 1, update_ts = now()
    //             where process_id = $1 and file_id = any($2)
    //         "#;
    //         self.client
    //             .execute(q_update, &[&process_id, &needs])
    //             .await
    //             .context("set REDISPATCHED")?;
    //         info!(count = needs.len(), "Zaznaczono REDISPATCHED");
    //     }
    //     Ok(needs)
    // }

    async fn mark_redispached_required_and_collect(
        &self,
        process_id: &str,
        max_retries: u32,
    ) -> Result<Vec<FileTask>> {
        // 1) wyłap kandydatów do retry (FAILED/TIMED_OUT, z limitem prób)
        let q_select = r#"
            select file_id, file_path, export_path
            from files
            where process_id = $1
              and status in ('FAILED','TIMED_OUT')
              and coalesce(retries, 0) < $2
        "#;
        let rows = self.client.query(q_select, &[&process_id, &(max_retries as i32)]).await?;

        let retry_ids: Vec<String> = rows.iter().map(|r| r.get::<_, String>(0)).collect();

        if retry_ids.is_empty() {
            return Ok(vec![]);
        }

        // 2) ustaw REDISPATCHED + ++retries
        let q_update = r#"
            update files
               set status = 'REDISPATCHED',
                   retries = coalesce(retries,0) + 1,
                   update_ts = now()
             where process_id = $1
               and file_id = any($2)
        "#;
        self.client.execute(q_update, &[&process_id, &retry_ids]).await?;

        // 3) zbuduj FileTask (mamy już ścieżki gotowe do ponownego dispatchu)
        let tasks = rows
            .into_iter()
            .map(|r| FileTask {
                file_id: r.get::<_, String>(0),
                file_path: r.get::<_, String>(1),
                export_path: r.get::<_, Option<String>>(2),
            })
            .collect();

        info!(count = retry_ids.len(), "Zaznaczono REDISPATCHED i zebrano do ponownego uruchomienia");
        Ok(tasks)
    }

    async fn add_incoming_if_absent(&self, process_id: &str, tasks: &[FileTask]) -> Result<()> {
        // Upsert wpisów plików dla tej sesji
        let q = r#"
            insert into files(process_id, file_id, file_path, export_path, status, create_ts, update_ts, retries)
            values($1,$2,$3,$4,'NEW', now(), now(), 0)
            on conflict (process_id, file_id) do nothing
        "#;
        for t in tasks {
            self.client
                .execute(q, &[&process_id, &t.file_id, &t.file_path, &t.export_path])
                .await?;
        }
        Ok(())
    }

    async fn status_of_all(&self, process_id: &str) -> Result<HashMap<String, Status>> {
        let q = r#"
            select file_id, status from files where process_id = $1
        "#;
        let rows = self.client.query(q, &[&process_id]).await?;
        let mut m = HashMap::new();
        for r in rows {
            let file_id: String = r.get(0);
            let status: String = r.get(1);
            m.insert(file_id, Status::from_str_loose(&status));
        }
        Ok(m)
    }

    async fn set_status(&self, process_id: &str, file_id: &str, status: Status) -> Result<()> {
        let q = r#"
            update files set status = $1, update_ts = now()
            where process_id = $2 and file_id = $3
        "#;
        self.client
            .execute(q, &[&status.as_str(), &process_id, &file_id])
            .await?;
        Ok(())
    }

    async fn write_final_meta_and_finish(
        &self,
        process_id: &str,
        export_path: &std::path::Path,
    ) -> Result<()> {
        // Pobieram basic_meta + specific_meta i zapisujemy meta.json
        let q = r#"
            select file_id, basic_meta, specific_meta
            from files
            where process_id = $1
        "#;
        let rows = self.client.query(q, &[&process_id]).await?;
        let mut meta: BTreeMap<String, serde_json::Value> = BTreeMap::new();
        for r in rows {
            let file_id: String = r.get(0);
            let basic: Option<serde_json::Value> = r.get(1);
            let spec: Option<serde_json::Value> = r.get(2);
            let mut obj = serde_json::Map::new();
            if let Some(b) = basic {
                obj.insert("basic_meta".into(), b);
            }
            if let Some(s) = spec {
                obj.insert("specific_meta".into(), s);
            }
            meta.insert(file_id, serde_json::Value::Object(obj));
        }
        let payload = serde_json::to_vec_pretty(&meta)?;
        tokio::fs::write(export_path, &payload)
            .await
            .with_context(|| format!("write {:?}", export_path))?;

        // Ustaw FINISHED dla wszystkich
        let q2 = r#"
            update files set status='FINISHED', update_ts = now()
            where process_id = $1 and status in ('COMPLETED')
        "#;
        self.client.execute(q2, &[&process_id]).await?;
        Ok(())
    }
    #[allow(dead_code)]
    async fn listen_channel(&self, channel: &str) -> Result<()> {
        let q = format!("LISTEN {}", channel);
        self.client.batch_execute(&q).await?;
        info!(%channel, "LISTEN ustawione");
        Ok(())
    }

}

// ======================== LISTENER (zgodnie z Twoim szkicem) ========================

/// Tworzymy strumień z `conn.poll_message(cx)` i forwardujemy do kanału mpsc.
async fn create_pg_listener_stream(
    cfg: &AppConfig,
    sender: UnboundedSender<tokio_postgres::AsyncMessage>,
) -> Result<()> {
    let dsn = if let Some(d) = &cfg.pg_dsn {
        d.clone()
    } else {
        format!(
            "host={} port={} user={} password={} dbname={}",
            cfg.pg_host.clone().unwrap_or_else(|| "127.0.0.1".into()),
            cfg.pg_port,
            cfg.pg_user.clone().unwrap_or_else(|| "postgres".into()),
            cfg.pg_password.clone().unwrap_or_default(),
            cfg.pg_dbname.clone().unwrap_or_else(|| "postgres".into())
        )
    };

    info!("Inicjuję połączenie (listener) z Postgres...");
    let (client, mut conn) = tokio_postgres::connect(&dsn, NoTls).await?;

    info!("Podłączam się pod strumień wiadomości z Postgres (LISTEN/NOTIFY)...");
    // LISTEN na kanale statusowym
    client
        .batch_execute(&format!("LISTEN {}", cfg.notify_channel))
        .await?;

    let stream = stream::poll_fn(move |cx| conn.poll_message(cx));
    // forward AsyncMessage do naszego kanału
    let forwarder = stream
        .map_err(|e| {
            error!(error=?e, "Problem z pobraniem wiadomości z Postgresa");
            e
        })
        .try_for_each(move |msg| {
            let _ = sender.send(msg);
            futures::future::ready(Ok(()))
        });

    tokio::spawn(async move {
        if let Err(e) = forwarder.await {
            error!(error=?e, "Listener zakończył się błędem");
        }
    });

    // Dodatkowy watchdog: co 30s SELECT 1, żeby utrzymać żywe połączenie
    let validate_client = Arc::new(client);
    let mut time_stream =
        IntervalStream::new(tokio::time::interval(Duration::from_secs(30)));
    let vc = validate_client.clone();
    tokio::spawn(async move {
        while time_stream.next().await.is_some() {
            match vc.query_one("SELECT 1", &[]).await {
                Ok(_) => debug!("Listener heartbeat OK"),
                Err(e) => warn!(error=?e, "Listener heartbeat FAILED"),
            }
        }
    });

    Ok(())
}

// ======================== DISPATCHER CORE ========================

struct Dispatcher {
    cfg: AppConfig,
    backend: Arc<dyn ProcessorBackend>,
    db: Option<Arc<Db>>, // None w dry-run
    dispatcher_run_id: Uuid,
    // dla ochrony przed duplikatami z NOTIFY
    old_queue_messages: Arc<DashSet<String>>,
}

impl Dispatcher {
    async fn new(cfg: AppConfig) -> Result<Self> {
        let dispatcher_run_id = Uuid::new_v4();

        // Wybór backendu
        let backend: Arc<dyn ProcessorBackend> = if cfg.dry_run {
            // tryb testowy
            Arc::new(DryRunBackend)
        } else {
            // REALNE POŁĄCZENIE Z CLOUD RUN
            let project = cfg
                .gcp_project
                .clone()
                .expect("parametr --gcp-project jest wymagany w trybie produkcyjnym");
            // region pobieramy z ENV (lub możesz dodać CLI: --gcp-region)
            let region = std::env::var("GCP_REGION")
                .unwrap_or_else(|_| "europe-central2".to_string());

            // tworzy backend GCP
            Arc::new(crate::cloud_run::CloudRunBackend::new(project, region).await?)
        };

        // Połączenie z bazą
        let db = if cfg.dry_run {
            None
        } else {
            Some(Arc::new(Db::connect(&cfg).await?))
        };


        Ok(Self {
            cfg,
            backend,
            db,
            dispatcher_run_id,
            old_queue_messages: Arc::new(DashSet::new()),
        })
    }

    async fn dispatch_stage_b(&self, tasks: &[FileTask]) -> Result<()> {
        info!("Rozpoczynam etap B (przetwarzanie właściwe)");

        // Grupowanie po rozszerzeniu
        let mut groups: HashMap<String, Vec<&FileTask>> = HashMap::new();
        for t in tasks {
            let ext = std::path::Path::new(&t.file_path)
                .extension()
                .and_then(|s| s.to_str())
                .unwrap_or("")
                .to_lowercase();
            groups.entry(ext).or_default().push(t);
        }

        for (ext, files) in groups {
            let Some(job) = self.cfg.ext_jobs.get(&ext).cloned() else {
                warn!(%ext, "Brak zdefiniowanego joba dla rozszerzenia – pomijam");
                continue;
            };

            for f in files {
                let payload = serde_json::json!({
                "process_id": self.cfg.process_id,
                "dispatcher_run_id": self.dispatcher_run_id.to_string(),
                "file_id": f.file_id,
                "file_path": f.file_path,
                "export_path": f.export_path,
            });

                let exec_id = self.backend.dispatch_job(&job, &payload).await?;
                info!(%exec_id, %job, "Cloud Run job uruchomiony");

                if let Some(db) = &self.db {
                    db.set_status(&self.cfg.process_id, &f.file_id, Status::Dispatched)
                        .await?;

                    // Tworze lokalne kopie, które są 'static (przechodzą do taska)
                    let db_clone = Arc::clone(db);
                    let pid = self.cfg.process_id.clone();
                    let fid = f.file_id.clone();
                    let backend = Arc::clone(&self.backend);
                    let exec_id = exec_id.clone();

                    // Uruchamiam monitorowanie w osobnym tasku
                    tokio::spawn(async move {
                        if let Some(cr) =
                            ProcessorBackend::as_any(backend.as_ref()).downcast_ref::<CloudRunBackend>()
                        {
                            match cr
                                .poll_until_done(
                                    &exec_id,
                                    Duration::from_secs(900), // timeout 15 min
                                    Duration::from_secs(10),  // polling co 10 s
                                )
                                .await
                            {
                                Ok(_) => {
                                    info!(%fid, "Execution SUCCEEDED");
                                    let _ = db_clone
                                        .set_status(&pid, &fid, crate::Status::Completed)
                                        .await;
                                }
                                Err(e) => {
                                    error!(%fid, error=?e, "Execution FAILED/TIMEOUT");
                                    let _ = db_clone
                                        .set_status(&pid, &fid, crate::Status::Failed)
                                        .await;
                                }
                            }
                        } else {
                            warn!("Backend nie wspiera monitorowania executions");
                        }
                    });
                }
            }
        }

        Ok(())
    }

    async fn load_payload(cli: &Cli) -> Result<Vec<FileTask>> {
        if let Some(json) = &cli.payload {
            let tasks: Vec<FileTask> = serde_json::from_str(json)
                .context("Błąd parsowania JSON payloadu")?;
            Ok(tasks)
        } else if let Some(path) = &cli.payload_file {
            let data = tokio::fs::read_to_string(path).await?;
            let tasks: Vec<FileTask> = serde_json::from_str(&data)
                .context("Błąd parsowania pliku payload")?;
            Ok(tasks)
        } else {
            Ok(vec![])
        }
    }

    fn merge_tasks_unique(mut base: Vec<FileTask>, mut extra: Vec<FileTask>) -> Vec<FileTask> {
        use std::collections::HashSet;
        let mut seen: HashSet<String> = base.iter().map(|t| t.file_id.clone()).collect();
        extra.retain(|t| !seen.contains(&t.file_id));
        base.extend(extra);
        base
    }

    async fn handle_pg_notifications(
        &self,
        mut receiver: UnboundedReceiver<tokio_postgres::AsyncMessage>,
    ) {
        let db = match &self.db {
            Some(db) => Arc::clone(db),
            None => {
                warn!("Brak DB – listener nieaktywny (dry_run)");
                return;
            }
        };

        while let Some(msg) = receiver.recv().await {
            if let tokio_postgres::AsyncMessage::Notification(n) = msg {
                let payload = n.payload();
                if self.old_queue_messages.contains(payload) {
                    debug!(%payload, "Duplikat NOTIFY – pomijam");
                    continue;
                }
                self.old_queue_messages.insert(payload.to_string());

                debug!(channel = n.channel(), %payload, "Odebrano NOTIFY");

                // Przykład payloadu: {"file_id":"123","status":"COMPLETED"}
                if let Ok(v) = serde_json::from_str::<serde_json::Value>(payload) {
                    if let (Some(file_id), Some(status)) =
                        (v.get("file_id").and_then(|v| v.as_str()), v.get("status").and_then(|v| v.as_str()))
                    {
                        let st = Status::from_str_loose(status);
                        if let Err(e) = db.set_status(&self.cfg.process_id, file_id, st).await {
                            error!(%file_id, error=?e, "Nie udało się zaktualizować statusu z NOTIFY");
                        } else {
                            info!(%file_id, %status, "Zaktualizowano status z NOTIFY");
                        }
                    } else {
                        warn!(%payload, "Nieprawidłowy format NOTIFY payload");
                    }
                } else {
                    warn!(%payload, "Nie udało się sparsować JSON z NOTIFY");
                }
            }
        }
        info!("Kanał NOTIFY zakończony (brak nowych wiadomości)");
    }

    async fn finalize_when_ready(&self) -> Result<()> {
        let db = match &self.db {
            Some(db) => Arc::clone(db),
            None => {
                warn!("Brak DB – finalizacja pominięta (dry_run)");
                return Ok(());
            }
        };

        let process_id = self.cfg.process_id.clone();
        let export_path = if let Some(ref p) = self.cfg.export_dir {
            p.clone()
        } else {
            std::path::PathBuf::from(format!("meta_{}.json", process_id))
        };

        let mut interval = tokio::time::interval(Duration::from_secs(90));
        loop {
            interval.tick().await;

            match Self::all_completed(&db, &process_id).await {
                Ok(true) => {
                    info!("Wszystkie pliki COMPLETED – uruchamiam finalizację meta.json");
                    if let Err(e) = db.write_final_meta_and_finish(&process_id, &export_path).await {
                        error!(error=?e, "Finalizacja meta.json nie powiodła się");
                    } else {
                        info!(path=?export_path, "Finalizacja zakończona pomyślnie (FINISHED)");
                    }
                    break;
                }
                Ok(false) => {
                    debug!("Nie wszystkie pliki ukończone – czekam dalej...");
                }
                Err(e) => {
                    warn!(error=?e, "Błąd sprawdzania statusów, spróbuję ponownie");
                }
            }
        }

        Ok(())
    }

    async fn poll_statuses_periodically(&self) -> Result<()> {
        let db = match &self.db {
            Some(db) => Arc::clone(db),
            None => {
                warn!("Brak DB – polling pominięty (dry_run)");
                return Ok(());
            }
        };

        let process_id = self.cfg.process_id.clone();
        let interval = self.cfg.poll_interval;

        info!(
            secs = interval.as_secs(),
            "Uruchamiam okresowy polling statusów (fallback)"
        );

        let mut ticker = tokio::time::interval(interval);
        loop {
            ticker.tick().await;

            match db.status_of_all(&process_id).await {
                Ok(statuses) if !statuses.is_empty() => {
                    let mut counts = std::collections::HashMap::new();
                    for s in statuses.values() {
                        *counts.entry(s.as_str()).or_insert(0usize) += 1;
                    }
                    debug!(?counts, "Statusy w pollingu");

                    // przykład: oznacz pliki, które długo wiszą w DISPATCHED
                    let dispatched_count = counts.get("DISPATCHED").copied().unwrap_or(0);
                    if dispatched_count > 0 {
                        info!(
                            dispatched_count,
                            "Niektóre pliki nadal w DISPATCHED – możliwy brak NOTIFY"
                        );
                    }
                }
                Ok(_) => {
                    debug!("Brak rekordów dla procesu w pollingu");
                }
                Err(e) => {
                    warn!(error=?e, "Polling statusów nieudany, spróbuję ponownie");
                }
            }
        }
    }

    async fn await_completion_and_exit(&self) -> Result<()> {
        let db = match &self.db {
            Some(db) => Arc::clone(db),
            None => {
                warn!("Brak DB – nie można monitorować zakończenia (dry_run)");
                return Ok(());
            }
        };

        let process_id = self.cfg.process_id.clone();
        info!("Uruchamiam watchdog zakończenia procesu (czeka na FINISHED/FAILED/TIMED_OUT)");

        let mut interval = tokio::time::interval(Duration::from_secs(30));

        loop {
            interval.tick().await;

            match Self::all_terminal(&db, &process_id).await {
                Ok(true) => {
                    info!("Wszystkie pliki osiągnęły status końcowy – kończę działanie dispatchera");
                    std::process::exit(0);
                }
                Ok(false) => {
                    debug!("Nie wszystkie pliki jeszcze zakończone – sprawdzę ponownie");
                }
                Err(e) => {
                    warn!(error=?e, "Błąd podczas sprawdzania statusów – spróbuję ponownie");
                }
            }
        }
    }

    async fn all_completed(db: &Db, process_id: &str) -> Result<bool> {
        let statuses = db.status_of_all(process_id).await?;
        if statuses.is_empty() {
            return Ok(false);
        }
        let all_done = statuses.values().all(|s| *s == Status::Completed);
        Ok(all_done)
    }

    async fn all_terminal(db: &Db, process_id: &str) -> Result<bool> {
        let statuses = db.status_of_all(process_id).await?;
        if statuses.is_empty() {
            return Ok(false);
        }
        let all_done = statuses.values().all(|s| {
            matches!(
                s,
                Status::Finished | Status::Failed | Status::TimedOut
            )
        });
        Ok(all_done)
    }

    async fn run(&self, tasks: Vec<FileTask>) -> Result<()> {
        let mut all_tasks = tasks.clone();

        if let Some(db) = &self.db {
            db.init_session(self.dispatcher_run_id, &self.cfg.process_id).await?;
            db.add_incoming_if_absent(&self.cfg.process_id, &tasks).await?;

            // uruchomienie listenera PG (LISTEN/NOTIFY)
            let (tx, rx) = unbounded_channel();
            if let Some(db) = &self.db {
                let cfg_clone = self.cfg.clone();
                let sender = tx.clone();
                tokio::spawn(async move {
                    if let Err(e) = create_pg_listener_stream(&cfg_clone, sender).await {
                        error!(error=?e, "Błąd uruchamiania listenera PG");
                    }
                });
            }

            // Uruchamiam task odbierający NOTIFY i aktualizujący statusy
            let this = self.clone_light();
            tokio::spawn(async move {
                this.handle_pg_notifications(rx).await;
            });


            // dołączam kandydatów do retry (FAILED/TIMED_OUT) – respektuj --max-retries
            let retry_tasks = db
                .mark_redispached_required_and_collect(&self.cfg.process_id, self.cfg.max_retries)
                .await?;
            if !retry_tasks.is_empty() {
                info!(count = retry_tasks.len(), "Dołączam pliki do retry");
                all_tasks = Self::merge_tasks_unique(all_tasks, retry_tasks);
            }
        }

        // uruchamiam etap B (dispatch plików do Cloud Run)
        self.dispatch_stage_b(&tasks).await?;

        // pinguje joby okresowo – jeśli --ping-interval-secs > 0
        if let Some(interval) = self.cfg.ping_interval {
            let backend = Arc::clone(&self.backend);
            let jobs: Vec<String> = self.cfg.ext_jobs.values().cloned().collect();

            tokio::spawn(async move {
                let mut ticker = tokio::time::interval(interval);
                loop {
                    ticker.tick().await;
                    for job in &jobs {
                        match backend.ping(job).await {
                            Ok(_) => debug!(%job, "Ping OK"),
                            Err(e) => warn!(%job, error=?e, "Ping failed"),
                        }
                    }
                }
            });
        }

        // uruchamiam task finalizacji meta.json, gdy wszystko będzie COMPLETED
        let this = self.clone_light();
        tokio::spawn(async move {
            if let Err(e) = this.finalize_when_ready().await {
                error!(error=?e, "Błąd w procesie finalizacji");
            }
        });

        // uruchamiam fallback polling co poll_interval_secs
        let this = self.clone_light();
        tokio::spawn(async move {
            if let Err(e) = this.poll_statuses_periodically().await {
                error!(error=?e, "Błąd w tasku pollingu");
            }
        });

        // uruchamiam watchdog zakończenia procesu
        let this = self.clone_light();
        tokio::spawn(async move {
            if let Err(e) = this.await_completion_and_exit().await {
                error!(error=?e, "Błąd w watchdogu zakończenia procesu");
            }
        });


        Ok(())
    }

    fn clone_light(&self) -> Self {
        Self {
            cfg: self.cfg.clone(),
            backend: Arc::clone(&self.backend),
            db: self.db.as_ref().map(Arc::clone),
            dispatcher_run_id: self.dispatcher_run_id,
            old_queue_messages: Arc::clone(&self.old_queue_messages),
        }
    }


}



// ======================== MAIN ========================

#[tokio::main]
async fn main() -> Result<()> {
    init_tracing();

    let cli = Cli::parse();
    let cfg: AppConfig = cli.clone().into();

    // wczytujem payload (lub pusta lista)
    let tasks = Dispatcher::load_payload(&cli).await?;
    if tasks.is_empty() {
        warn!("Brak zadań w payloadzie – nic do zrobienia (to nie jest błąd).");
    }

    // start
    let dispatcher = Dispatcher::new(cfg).await?;
    if let Err(e) = dispatcher.run(tasks).await {
        error!(error=?e, "Dispatcher zakończony błędem");
        // zgodnie z założeniem – wyjście != 0, bez „wiszenia”
        std::process::exit(1);
    }
    Ok(())
}

fn init_tracing() {
    use tracing_subscriber::{fmt, EnvFilter};
    let filter = EnvFilter::try_from_default_env()
        .unwrap_or_else(|_| EnvFilter::new("info,udr_dispatcher=info"));
    tracing_subscriber::registry()
        .with(filter)
        .with(fmt::layer().with_target(false))
        .init();
}
